{"cells":[{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1589,"status":"ok","timestamp":1682366556920,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"dyc7om86kOFs","outputId":"eed8d423-ea0f-444c-969e-53a133089aca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31717,"status":"ok","timestamp":1682312067942,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"oMOKMwDhkOnB","outputId":"4270f74a-eae3-4663-9ad3-a67dc283d14d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting boto3\n","  Downloading boto3-1.26.118-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.118\n","  Downloading botocore-1.29.118-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3) (1.26.15)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.118->boto3) (1.16.0)\n","Installing collected packages: jmespath, botocore, s3transfer, boto3\n","Successfully installed boto3-1.26.118 botocore-1.29.118 jmespath-1.0.1 s3transfer-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SentencePiece==0.1.94\n","  Downloading sentencepiece-0.1.94-cp39-cp39-manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: SentencePiece\n","Successfully installed SentencePiece-0.1.94\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["! pip install boto3\n","! pip install SentencePiece==0.1.94\n","! pip install transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1682344992218,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"Y6Ok2gPYkRpR","outputId":"38739f08-f9ce-4b99-8fa7-7dbd88f8914e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1gvgdEyQQFFN43xnL2_DdI4rUtMI5gnmU/transformer-drg-style-transfer-master\n"]}],"source":["%cd /content/drive/MyDrive/transformer-drg-style-transfer-master/\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RTtAtl9xkJTB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682345004724,"user_tz":420,"elapsed":9659,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"2b7e1ffb-3608-48d3-dede-369cf1b1ab5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"]}],"source":["import torch, os\n","from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n","from tqdm import tqdm\n","import numpy as np\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n","from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n","#from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n","from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"]},{"cell_type":"code","source":["from tqdm.auto import tqdm"],"metadata":{"id":"zChPoS1sHPVG","executionInfo":{"status":"ok","timestamp":1682345005412,"user_tz":420,"elapsed":691,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pNGDEHEkJTD"},"source":["## Delete and Generate"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12031,"status":"ok","timestamp":1682345017437,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"oxdG81iukJTD","outputId":"307af729-20c5-49d8-e736-bee304a8b50c"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_pretrained_bert.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"]}],"source":["special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>'] # Set the special tokens\n","tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7DDUUn4WkJTE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682345026670,"user_tz":420,"elapsed":9237,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"26132784-cf2a-41b5-8dea-152b4b2330b2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["OpenAIGPTLMHeadModel(\n","  (transformer): OpenAIGPTModel(\n","    (tokens_embed): Embedding(40483, 768)\n","    (positions_embed): Embedding(512, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x Block(\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_1): BertLayerNorm()\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): BertLayerNorm()\n","      )\n","    )\n","  )\n","  (lm_head): OpenAIGPTLMHead(\n","    (decoder): Linear(in_features=768, out_features=40483, bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":5}],"source":["#path = os.path.join(os.getcwd(), \"./log_yelp_bert_best_head_preprocessed/pytorch_model_zero_grad_1.bin\") ## Model Path\n","path = \"/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_tfidf/model_output/pytorch_model_zero_grad_1.bin\"\n","model_state_dict = torch.load(path, map_location=device)\n","model.load_state_dict(model_state_dict)\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration\n","tokenizer_cls = T5Tokenizer.from_pretrained('t5-base', max_length=70)\n","model_cls =  T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/', return_dict = True)\n","model_cls.load_state_dict(torch.load('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/pytorch_model.bin', map_location=torch.device(device)))\n","model_cls.eval()\n","model_cls.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZpPPBOPGdd9","executionInfo":{"status":"ok","timestamp":1682345040374,"user_tz":420,"elapsed":13724,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"d3fa67f3-a603-4a69-d104-d4d139468fb9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"U_xzPDqukJTE","executionInfo":{"status":"ok","timestamp":1682302882279,"user_tz":420,"elapsed":26,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["max_seq_len=70\n","sm = torch.nn.Softmax(dim=-1)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682345040374,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"yFGbefemkJTE","outputId":"170f6703-24ce-4f5b-ef19-a83889f060d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{},"execution_count":7}],"source":["model.config.n_positions"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Icla8od6kJTE","executionInfo":{"status":"ok","timestamp":1682345040374,"user_tz":420,"elapsed":17,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def preditction_with_beam_search(ref_text, beam_width=3, vocab_length=40483):\n","    \"\"\"\n","    This function decodes sentences using Beam Seach. \n","    It will output #sentences = beam_width. This function works on a single example.\n","    \n","    ref_text : string : Input sentence\n","    beam_width : int : Width of the output beam\n","    vocab_length : int : Size of the Vocab after adding the special tokens\n","    \"\"\"\n","    \n","    done = [False for i in range(beam_width)] # To track which beams are already decoded\n","    stop_decode = False\n","    decoded_sentences=[] # List of decoded sentences at any given time\n","    \n","    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n","    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n","    \n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n","    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n","\n","    #index_tokens = [indexed_tokens for i in range(beam_width)]\n","    torch_tensor = torch.tensor(index_tokens).to(device)\n","    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n","    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n","    count = 0\n","    while count < model.config.n_positions and not stop_decode:\n","        if count == 0: # For the first step when only one sentence is availabe\n","            with torch.no_grad():\n","                # Calculate output probability distribution over the Vocab,\n","                preds = sm(model(torch_tensor)) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n","            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n","            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n","            # Update the best_scores, for first time just add the topk values directly\n","            for i in range(beam_width):\n","                best_scoes[i] = top_v[0][i].item()\n","            count += 1\n","        else: # After first step\n","            # Prepare the current_state by concating original input and decoded beam indexes\n","            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n","            # Prediction on the current state\n","            with torch.no_grad():\n","                preds = sm(model(current_state))\n","            # Multiply new probability predictions with corresponding best scores\n","            # Total socres = beam_width * Vocab_Size\n","            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n","            # Fatch the top scores and indexes \n","            vals, inx = flatten_score.topk(beam_width)\n","            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n","            best_scoes_inx = (inx / vocab_length).tolist()\n","            best_scoes = vals.tolist()\n","            # Unflatten the index \n","            correct_inx = (inx % vocab_length).tolist()\n","            \n","            # Check if done for all the Beams\n","            for i in range(beam_width):\n","                if correct_inx[i] == tokenizer.special_tokens[\"<END>\"]:\n","                    done[i] = True\n","            # Update the best score for each the current Beams\n","            for i in range(beam_width):\n","                if not done[i]:\n","                    best_scoes[i] = vals.tolist()[i]\n","            # Check is All the Beams are Done\n","            if (sum(done) == beam_width):\n","                stop_decode = True\n","            # Prepapre the new beams\n","            temp_lt=[0 for i in range(beam_width)]\n","            for i,x in enumerate(best_scoes_inx):\n","                temp_lt[i] = beam_indexes[int(x)] + [correct_inx[i]]\n","            # Update the Beam indexes\n","            beam_indexes = temp_lt\n","            del temp_lt\n","            count += 1\n","    # Decode All the beam indexes to till <END> token only and convert into sentence\n","    for i in range(beam_width):\n","        try:\n","            end_index = beam_indexes[i].index(tokenizer.special_tokens[\"<END>\"])\n","        except ValueError:\n","            end_index = len(beam_indexes[i])\n","            \n","        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n","        \n","    return decoded_sentences"]},{"cell_type":"code","source":[],"metadata":{"id":"XfqGpky1EoIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Q_kMEePrGEk8","executionInfo":{"status":"ok","timestamp":1682345040374,"user_tz":420,"elapsed":16,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def get_best_sentence(input_sentences, sentiment= 0):\n","    \n","    \"\"\"\n","    This function selects the sentence from the Beam of the sentences,\n","    based on the classification label.\n","\n","    input_sentences : list of strings : Sentences generated by the Beam search decoding\n","    sentiment: int : Expected sentiment (in general class for the classification)\n","    model_name_or_path: str : HuggingFace model name or path to T5 model\n","    \"\"\"\n","\n","    # Generate input prompt for T5\n","    prompt = f\"classify the sentiment of the following sentences as {sentiment}. sentence:\"\n","\n","    # Create batch inputs for T5 model\n","    input_ids = []\n","    input_masks = []\n","    for sen in input_sentences:\n","        input_str = f\"{prompt} {sen}\"\n","        input_mask = tokenizer_cls.encode_plus(input_str, truncation=True, pad_to_max_length=True,max_length=70, )\n","        input_masks.append(input_mask['attention_mask'])\n","        input_ids.append(input_mask['input_ids'])\n","        #input_ids.append(tokenizer_cls.encode(input_str, truncation=True, max_length=70))\n","      \n","    # print(input_ids)\n","    # print(\"Input ids shape:\", len(input_ids), len(input_ids[0]))\n","    # print(\"Input masks shape:\", len(input_masks), len(input_ids[0]))\n","\n","    input_ids = torch.tensor(input_ids).to(model_cls.device)\n","    #print(input_sentences)\n","    #input_masks = torch.tensor(input_masks).to(model_cls.device)\n","\n","\n","    #Predict sentiment labels using T5\n","    # with torch.no_grad():\n","    #     outputs = model_cls.generate(input_ids=input_ids, decoder_input_ids = input_ids , max_length=2)\n","\n","\n","\n","    # # Decode predicted sentiment labels\n","    # preds = [tokenizer_cls.decode(output, skip_special_tokens=True) for output in outputs]\n","    # output_sentences = [sen.strip() for sen in preds]\n","\n","    with torch.no_grad():\n","        logits = model_cls(input_ids=input_ids, decoder_input_ids=input_ids)\n","        logits = logits.logits[:, 1]\n","        softmaxed_logits = torch.softmax(logits, dim=-1)\n","\n","    scores = softmaxed_logits\n","    preds = scores.tolist()\n","    inx, inx_val = None, 0\n","    for i in range(len(input_sentences)):\n","        temp = preds[i][1-sentiment]\n","        if temp > inx_val:\n","            inx = i\n","            inx_val = temp\n","    return input_sentences[inx]\n","    #Find the sentence with the highest sentiment score\n","    best_sentence_index = max(range(len(output_sentences)), key=lambda i: scores[i])\n","\n","\n","    return input_sentences[inx]\n"]},{"cell_type":"markdown","metadata":{"id":"m3vIKAJikJTG"},"source":["## Example"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25183,"status":"ok","timestamp":1682312166073,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"qT67SZwUkJTG","outputId":"c558048f-5654-4a01-c1b2-db55437a11ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['it is not terrible , but it is very good .',\n"," 'it is not terrible , but it is very good .',\n"," 'it is not terrible , but it is very good .',\n"," 'it is not terrible , but it is very good .']"]},"metadata":{},"execution_count":12}],"source":["op = preditction_with_beam_search(\"<POS> <CON_START> it is not terrible , but it is very good . <START>\",4)\n","op"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":2633,"status":"ok","timestamp":1682312168685,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"5ht4Ut6XkJTG","outputId":"cd498041-bcc6-4fd0-9599-7dae9223370d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input ids shape: 4 26\n","Input masks shape: 4 26\n"]},{"output_type":"stream","name":"stderr","text":["Input length of decoder_input_ids is 26, but `max_length` is set to 2. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"]},{"output_type":"execute_result","data":{"text/plain":["'it is not terrible , but it is very good .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["get_best_sentence(op)"]},{"cell_type":"markdown","metadata":{"id":"uVZqFEJ7kJTG"},"source":["### Predictions for the reference files"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1682345067373,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"pgA-HN8KkJTG","outputId":"0a4554e3-77e1-4097-8fa2-84248bf52309"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt',\n"," 'reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt',\n"," 'reference_1_predictions_with_beam_search.txt',\n"," 'reference_1.txt',\n"," 'reference_0.txt',\n"," 'reference_0_predictions_with_beam_search.txt']"]},"metadata":{},"execution_count":11}],"source":["output_dir = \"/content/drive/MyDrive/transformer-drg-style-transfer-master/outputs_drg_yelp\"\n","os.listdir(output_dir)"]},{"cell_type":"code","source":["reference_0 = '/content/drive/MyDrive/transformer-drg-style-transfer-master/outputs_drg_yelp/reference_0.txt'\n","reference_1 = '/content/drive/MyDrive/transformer-drg-style-transfer-master/outputs_drg_yelp/reference_1.txt'"],"metadata":{"id":"y9pJUft8R79j","executionInfo":{"status":"ok","timestamp":1682345068225,"user_tz":420,"elapsed":106,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN3WJWvQkJTG"},"outputs":[],"source":["with open(os.path.join(output_dir,\"reference_0_predictions_with_beam_search.txt\") ,\"w+\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open(reference_0, 'r') as fp:\n","        for line in tqdm(fp):\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            #print(out_sen)\n","            print(c,get_best_sentence(out_sen, sentiment = 1))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHc0mHxvkJTG"},"outputs":[],"source":["with open(os.path.join(output_dir,\"reference_1_predictions_with_beam_search.txt\") ,\"w+\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open(reference_1, 'r') as fp:\n","        for line in tqdm(fp):\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length = max(tokenizer.special_tokens.values()) + 1)\n","            print(c,get_best_sentence(out_sen, sentiment = 0))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"GUvxuVtrkJTG"},"source":["## Delete, Retrieve and Generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKXwtKwWkJTG"},"outputs":[],"source":["special_tokens = ['<ATTR_WORDS>','<CON_START>','<START>','<END>'] # Set the special tokens\n","tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2HOqElmkJTH"},"outputs":[],"source":["#path = os.path.join(os.getcwd(), \"./log_yelp_retireve_edit_bert_best_head_preprocessing/pytorch_model_zero_grad_1.bin\")\n","path = \"/content/drive/MyDrive/transformer-drg-style-transfer-master (1)/model_output/drg_output/pytorch_model_zero_grad_1.bin\"\n","model_state_dict = torch.load(path, map_location=device)\n","model.load_state_dict(model_state_dict)\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1679893254180,"user":{"displayName":"Abhishek Upmanyu","userId":"02426381350378742750"},"user_tz":420},"id":"pF7zHYAikJTH","outputId":"8f938d8c-6421-40bc-e000-d45059e7b9cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['reference_1.txt',\n"," 'reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt',\n"," 'reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt']"]},"metadata":{},"execution_count":71}],"source":["# Output dir have reference files generated using TFIDF for retrieve attributes from opposite corpus\n","output_dir = '/content/drive/MyDrive/transformer-drg-style-transfer-master (1)/data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf'\n","os.listdir(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bLKNttRkJTH"},"outputs":[],"source":["with open(os.path.join(output_dir,\"reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open() as fp:\n","        for line in fp:\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            print(c,get_best_sentence(out_sen, sentiment = 1))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"]},{"cell_type":"code","source":["# Output dir have reference files generated using TFIDF for retrieve attributes from opposite corpus\n","output_dir = '/content/drive/MyDrive/transformer-drg-style-transfer-master (1)/outputs_drg_yelp'\n","os.listdir(output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8-3heZIbV-k","executionInfo":{"status":"ok","timestamp":1679893105705,"user_tz":420,"elapsed":5,"user":{"displayName":"Abhishek Upmanyu","userId":"02426381350378742750"}},"outputId":"c56d02d6-7bbe-4076-bec5-b94bd344299c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"5Xorg0DYkJTH","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"error","timestamp":1682366549249,"user_tz":420,"elapsed":462,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"191b7a51-e754-44fa-d8d3-8de9a7995ead"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-4c0e07bcf94a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/transformer-drg-style-transfer-master (1)/data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/reference_1.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mout_sen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreditction_with_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/transformer-drg-style-transfer-master (1)/data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/reference_1.txt'"]}],"source":["with open(os.path.join(output_dir,\"reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open('/content/drive/MyDrive/transformer-drg-style-transfer-master (1)/data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/reference_1.txt') as fp:\n","        for line in fp:\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            print(c,get_best_sentence(out_sen, sentiment = 0))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"]},{"cell_type":"code","source":[],"metadata":{"id":"civLupkzW4K4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":0}