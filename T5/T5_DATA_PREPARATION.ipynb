{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuhAc4ZtIJPvavwNWcmKKL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"c61faa7757394c94bf626d1705f049c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ceebcd2837df44d184b014a62fcbec05","IPY_MODEL_3321e95429e0405395da6581ecf00898","IPY_MODEL_720f96d33ed64db88c8ddbdfba206028"],"layout":"IPY_MODEL_ffea86a221b2495fab6baa7f4cd8c491"}},"ceebcd2837df44d184b014a62fcbec05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dae1f8f14e4544f7b62735d492992222","placeholder":"​","style":"IPY_MODEL_0cd7323bb26d40278cb0be87b6a2b713","value":"Downloading (…)ve/main/spiece.model: 100%"}},"3321e95429e0405395da6581ecf00898":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10e266a7dc0c496bbc591b6589fe8679","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69e3b630435949bfb1505f792aa77932","value":791656}},"720f96d33ed64db88c8ddbdfba206028":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a4e96ea17df4b3bbb8b82f906c5b14b","placeholder":"​","style":"IPY_MODEL_a743d644490b48ea878b0de5f48a0ad9","value":" 792k/792k [00:00&lt;00:00, 8.01MB/s]"}},"ffea86a221b2495fab6baa7f4cd8c491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dae1f8f14e4544f7b62735d492992222":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cd7323bb26d40278cb0be87b6a2b713":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10e266a7dc0c496bbc591b6589fe8679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69e3b630435949bfb1505f792aa77932":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a4e96ea17df4b3bbb8b82f906c5b14b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a743d644490b48ea878b0de5f48a0ad9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a69cd16f39164ae796d2d4b86351ab4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ace080e790274b11a2c5db48efd62233","IPY_MODEL_050274601d5449ce87bbc6a2304460ea","IPY_MODEL_e1cad9e23ec74d75bd6300a0ccc00623"],"layout":"IPY_MODEL_1f9f725f196f44dea9a1b9e82cd1bb19"}},"ace080e790274b11a2c5db48efd62233":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d89e0dc17c8f499e940d55a9c42fa674","placeholder":"​","style":"IPY_MODEL_4fb66f0076594aeeb09585cac97cf9dc","value":"Downloading (…)lve/main/config.json: 100%"}},"050274601d5449ce87bbc6a2304460ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b17cc29a1b6c414d817d63afa4d2da3d","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b67fea0a494b49b28c7d4a3d3652f01b","value":1208}},"e1cad9e23ec74d75bd6300a0ccc00623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24cf4c80745b421b8c4d8c5eb1e36a59","placeholder":"​","style":"IPY_MODEL_a8d67a6bcc134e5184340f745cc5222a","value":" 1.21k/1.21k [00:00&lt;00:00, 38.5kB/s]"}},"1f9f725f196f44dea9a1b9e82cd1bb19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d89e0dc17c8f499e940d55a9c42fa674":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb66f0076594aeeb09585cac97cf9dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b17cc29a1b6c414d817d63afa4d2da3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b67fea0a494b49b28c7d4a3d3652f01b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24cf4c80745b421b8c4d8c5eb1e36a59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d67a6bcc134e5184340f745cc5222a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfzhLRswlx5L","executionInfo":{"status":"ok","timestamp":1682209666923,"user_tz":420,"elapsed":36802,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"a8e20578-bad1-415b-c298-29a58e991e07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/transformer-drg-style-transfer-master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQVocY00nDZ4","executionInfo":{"status":"ok","timestamp":1682209666924,"user_tz":420,"elapsed":11,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"6f82aa4a-7e2c-4c4d-8a6f-a98f43905fe7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1gvgdEyQQFFN43xnL2_DdI4rUtMI5gnmU/transformer-drg-style-transfer-master\n"]}]},{"cell_type":"code","source":["! pip install boto3\n","! pip install SentencePiece==0.1.94\n","! pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUB_OasgpLO-","executionInfo":{"status":"ok","timestamp":1682209695197,"user_tz":420,"elapsed":28278,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"33e24330-ed40-4daf-9bdd-ed4b9351c739"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting boto3\n","  Downloading boto3-1.26.118-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.118\n","  Downloading botocore-1.29.118-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3) (1.26.15)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.118->boto3) (1.16.0)\n","Installing collected packages: jmespath, botocore, s3transfer, boto3\n","Successfully installed boto3-1.26.118 botocore-1.29.118 jmespath-1.0.1 s3transfer-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SentencePiece==0.1.94\n","  Downloading sentencepiece-0.1.94-cp39-cp39-manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: SentencePiece\n","Successfully installed SentencePiece-0.1.94\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"code","source":["import csv\n","import logging\n","import os\n","import random\n","import sys\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration"],"metadata":{"id":"cDMXqM62o9Vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# file paths\n","data_dir = \"/content/drive/MyDrive/transformer-drg-style-transfer-master/data/\"\n","dataset = \"yelp\" # amazon / yelp / imagecaption\n","train_0 = os.path.join(data_dir ,\"{}/sentiment.train.0\".format(dataset))\n","train_1 = os.path.join(data_dir,\"{}/sentiment.train.1\".format(dataset))\n","test_0 = os.path.join(data_dir,\"{}/sentiment.test.0\".format(dataset))\n","test_1 = os.path.join(data_dir,\"{}/sentiment.test.1\".format(dataset))\n","dev_0 = os.path.join(data_dir,\"{}/sentiment.dev.0\".format(dataset))\n","dev_1 = os.path.join(data_dir,\"{}/sentiment.dev.1\".format(dataset))\n","reference_0 = os.path.join(data_dir,\"{}/reference.0\".format(dataset))\n","reference_1 = os.path.join(data_dir,\"{}/reference.1\".format(dataset))"],"metadata":{"id":"7eows-7spcKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# file paths\n","data_dir = \"/content/drive/MyDrive/transformer-drg-style-transfer-master/data/\"\n","dataset = \"yelp\" # amazon / yelp / imagecaption\n","train_0_out = os.path.join(data_dir ,\"{}/processed_files_with_t5_with_best_head/sentiment.train.0\".format(dataset))\n","train_1_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/sentiment.train.1\".format(dataset))\n","test_0_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/sentiment.test.0\".format(dataset))\n","test_1_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/sentiment.test.1\".format(dataset))\n","dev_0_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/sentiment.dev.0\".format(dataset))\n","dev_1_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/sentiment.dev.1\".format(dataset))\n","reference_0_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/reference.0\".format(dataset))\n","reference_1_out = os.path.join(data_dir,\"{}/processed_files_with_t5_with_best_head/reference.1\".format(dataset))"],"metadata":{"id":"TtCsSoSdp2AB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reference_1_out"],"metadata":{"id":"dASqZfkthnqq","executionInfo":{"status":"ok","timestamp":1682209943880,"user_tz":420,"elapsed":596,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"5b130187-d3e5-4982-d431-4ceb491a6fe5","colab":{"base_uri":"https://localhost:8080/","height":53}},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/reference.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["## Model for performing Classification\n","tokenizer = T5Tokenizer.from_pretrained('t5-base', max_length=70)\n","model =  T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/', return_dict = True)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/pytorch_model.bin', map_location=torch.device(device)))\n","model.eval()\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c61faa7757394c94bf626d1705f049c6","ceebcd2837df44d184b014a62fcbec05","3321e95429e0405395da6581ecf00898","720f96d33ed64db88c8ddbdfba206028","ffea86a221b2495fab6baa7f4cd8c491","dae1f8f14e4544f7b62735d492992222","0cd7323bb26d40278cb0be87b6a2b713","10e266a7dc0c496bbc591b6589fe8679","69e3b630435949bfb1505f792aa77932","0a4e96ea17df4b3bbb8b82f906c5b14b","a743d644490b48ea878b0de5f48a0ad9","a69cd16f39164ae796d2d4b86351ab4a","ace080e790274b11a2c5db48efd62233","050274601d5449ce87bbc6a2304460ea","e1cad9e23ec74d75bd6300a0ccc00623","1f9f725f196f44dea9a1b9e82cd1bb19","d89e0dc17c8f499e940d55a9c42fa674","4fb66f0076594aeeb09585cac97cf9dc","b17cc29a1b6c414d817d63afa4d2da3d","b67fea0a494b49b28c7d4a3d3652f01b","24cf4c80745b421b8c4d8c5eb1e36a59","a8d67a6bcc134e5184340f745cc5222a"]},"id":"39oAYjg_tgte","executionInfo":{"status":"ok","timestamp":1682209729588,"user_tz":420,"elapsed":29241,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"4eb3e581-ceb5-4f50-ecc7-32f721cb748e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61faa7757394c94bf626d1705f049c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69cd16f39164ae796d2d4b86351ab4a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# tokenizer = T5Tokenizer.from_pretrained('t5-base', max_length=70)\n","# model =  T5ForConditionalGeneration.from_pretrained('t5-base', return_dict = True)\n","# #model_cls.load_state_dict(torch.load('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/pytorch_model.bin', map_location=torch.device(device)))\n","# model.eval()"],"metadata":{"id":"rKP-21SdvGtt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_words=['is','are','was','were','has','have','had','a','an','the','this','that','these','those','there','how','i','we',\n","             'he','she','it','they','them','their','his','him','her','us','our', 'and','in','my','your','you', 'will', 'shall']\n","common_words_tokens = tokenizer.convert_tokens_to_ids(common_words)\n","not_to_remove_ids = tokenizer.convert_tokens_to_ids(['</s>', \".\", \"?\", \"!\"])\n","not_to_remove_ids += common_words_tokens"],"metadata":{"id":"tgVz9cQ4vRxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_output_file(original_sentences,processed_sentences, output_file, sentiment=\"<POS>\"):\n","    print(output_file)\n","    with open(output_file,\"w\") as fp:\n","        for sen1,sen2 in zip(original_sentences,processed_sentences):\n","            if sen1 != None and sen2 != None:\n","                str1 = sentiment + \" <CON_START> \" + sen2 + \" <START> \" + sen1 + \" <END>\\n\"\n","                fp.write(str1)"],"metadata":{"id":"g9tV24ZmwS8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_ref_output_file(processed_sentences, output_file,sentiment=\"<POS>\"):\n","    with open(output_file,\"w\") as fp:\n","        for sen in tqdm(processed_sentences):\n","            if sen != None:\n","                str1 = sentiment + \" <CON_START> \" + sen + \" <START>\\n\"\n","                fp.write(str1)"],"metadata":{"id":"N1IAuz6FB9XZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def concate_files(inp_files, out_files):\n","    with open(out_files,\"w\") as fp:\n","        for file in inp_files:\n","            with open(file) as f:\n","                for line in f:\n","                    fp.write(line)"],"metadata":{"id":"c3zI3bNaCA0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import psutil\n","\n","def clear_ram_cache():\n","    \"\"\"\n","    Clears the RAM cache on a Linux-based system.\n","    \"\"\"\n","    os.system(\"sync; echo 1 > /proc/sys/vm/drop_caches\")\n","    #print(\"RAM cache cleared.\")\n","\n","clear_ram_cache()"],"metadata":{"id":"YMl8j8MBeYVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_file(file_path):\n","    with open(file_path) as fp:\n","        data = fp.read().splitlines()\n","    return data"],"metadata":{"id":"l3XmD8MzbTtd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_0_data = read_file(train_0)\n","train_1_data = read_file(train_1)\n","dev_0_data = read_file(dev_0)\n","dev_1_data = read_file(dev_1)\n","test_0_data = read_file(test_0)\n","test_1_data = read_file(test_1)\n","ref_0_data = read_file(reference_0)\n","ref_1_data = read_file(reference_1)"],"metadata":{"id":"-HDnH8iKRVKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_seq_len = 72\n","def run_attn_examples(input_sentences, layer, head, batch_size=128):\n","    \"\"\"\n","    Returns Attention weights for selected Layer and Head along with ids and tokens\n","    of the input_sentence\n","    \"\"\"\n","    dataset = tokenizer(input_sentences, max_length=72, pad_to_max_length=True, \n","                                                          return_tensors=\"pt\", truncation = True)\n","    dataset = dataset['input_ids']\n","    \n","    \n","    model.to(device)\n","    attention_weights = [None for z in input_sentences]\n","    idx = 0\n","    tokens_to_decode = [None for k in range(len(input_sentences))]\n","    ids_to_decode = [None for k in range(len(input_sentences))]\n","    for j,sen in enumerate(tqdm(input_sentences)):\n","\n","      text_tokens = tokenizer.tokenize(sen)\n","      if len(text_tokens) >= max_seq_len-2:\n","          text_tokens = text_tokens[:max_seq_len-4]\n","      tokens =  text_tokens + [\"</s>\"]\n","      tokens_to_decode[j] = tokens\n","      temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n","      #temp_ids = tokenizer(sen)['input_ids']\n","      ids_to_decode[j] = temp_ids\n","    \n","\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    it = iter(loader)\n","      \n","\n","    with torch.no_grad():\n","        for batch in tqdm(it):\n","            batch_attn_probs = []  # To store attention weights for each layer in the current batch\n","            torch.cuda.empty_cache()\n","            input_ids = batch.to(device)\n","            outputs = model(input_ids=input_ids, output_attentions=True, decoder_input_ids=input_ids)\n","            attn = outputs.cross_attentions\n","            for j in range(len(attn[layer])):\n","              attention_weights[idx * batch_size + j] = (attn[layer][j][head][0]).to('cpu')\n","            idx+=1\n","\n","    return attention_weights, dataset, tokens_to_decode\n","\n","\n","\n","\n","# def run_attn_examples(input_sentences, layer, head, batch_size=128):\n","#     \"\"\"\n","#     Returns Attention weights for selected Layer and Head along with ids and tokens\n","#     of the input_sentence\n","#     \"\"\"\n","#     ids = []\n","#     ids_to_decode = [None for k in range(len(input_sentences))]\n","#     tokens_to_decode = [None for k in range(len(input_sentences))]\n","#     segment_ids = []\n","#     input_masks = []\n","#     attention_weights = [None for z in input_sentences]\n","#     ## BERT pre-processing\n","#     for j,sen in enumerate(tqdm(input_sentences)):\n","        \n","#         text_tokens = tokenizer(sen)['input_ids']\n","#         if len(text_tokens) >= max_seq_len-2:\n","#             text_tokens = text_tokens[:max_seq_len-4]\n","#         tokens = text_tokens + [\"</s>\"]\n","#         tokens_to_decode[j] = tokens\n","#         temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n","#         ids_to_decode[j] = temp_ids\n","#         input_mask = [1] * len(temp_ids)\n","#         segment_id = [0] * len(temp_ids)\n","#         padding = [0] * (max_seq_len - len(temp_ids))\n","        \n","        \n","#         temp_ids += padding\n","#         input_mask += padding\n","#         segment_id += padding\n","        \n","#         ids.append(temp_ids)\n","#         input_masks.append(input_mask)\n","#         segment_ids.append(segment_id)\n","    \n","#     # Convert Ids to Torch Tensors\n","#     ids = torch.tensor(ids) \n","#     segment_ids = torch.tensor(segment_ids)\n","#     input_masks = torch.tensor(input_masks)\n","    \n","#     steps = len(ids) // batch_size\n","#     idx = 0\n","#     for i in trange(steps+1):\n","#         if i == steps:\n","#             temp_ids = ids[i * batch_size : len(ids)]\n","#             temp_segment_ids = segment_ids[i * batch_size: len(ids)]\n","#             temp_input_masks = input_masks[i * batch_size: len(ids)]\n","#         else:\n","#             temp_ids = ids[i * batch_size : i * batch_size + batch_size]\n","#             temp_segment_ids = segment_ids[i * batch_size: i * batch_size + batch_size]\n","#             temp_input_masks = input_masks[i * batch_size: i * batch_size + batch_size]\n","        \n","#         temp_ids = temp_ids.to(device)\n","#         temp_segment_ids = temp_segment_ids.to(device)\n","#         temp_input_masks = temp_input_masks.to(device)\n","#         with torch.no_grad():\n","#              outputs = model(temp_ids, temp_segment_ids, temp_input_masks, output_attentions=True)\n","#              attn = outputs.cross_attentions\n","             \n","             \n","#         for j in range(len(attn[layer])):\n","#             #print(len(attn[layer][j]))\n","#             attention_weights[i * batch_size + j] = (attn[layer][j][head][1]).to('cpu')\n","    \n","#     return attention_weights, ids_to_decode, tokens_to_decode"],"metadata":{"id":"6HOmzmTZEz3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YqUCAZQmiq2"},"outputs":[],"source":["def prepare_data(aw, ids_to_decode, tokens_to_decode):\n","    out_sen = [None for i in range(len(aw))]\n","    for i in tqdm(range(len(aw))): \n","        clear_ram_cache()\n","        #topv, topi = aw[i].topk(len(inps_tokens[i]))\n","        \n","        p = ids_to_decode[i].numpy().tolist()\n","       \n","        \n","        topv, topi = aw[i].topk(p.index(0))\n","        topi = topi.tolist()\n","        topv = topv.tolist()\n","        \n","        #print(\"Original Top Indexes = {}\".format(topi))\n","        \n","        topi = [topi[j] for j in range(len(topi)) if ids_to_decode[i][topi[j]] not in not_to_remove_ids] # remove noun and common words\n","        #print(\"After removing Nouns = {}\".format(topi))\n","        \n","        #topi = [topi[j] for j in range(len(topi)) if \"|\" not in tokens_to_decode[i][topi[j]]]\n","        #topi = [topi[j] for j in range(len(topi)) if \"|\" not in tokens_to_decode[i][topi[j]]]\n","         # Remove half words\n","        #print(\"After removing Half-words = {}\".format(topi))\n","\n","        if (len(topi) < 4 and len(topi) > 0):\n","            topi = [topi[0]]\n","        elif(len(topi) < 8):\n","            topi = topi[:2]\n","        else:\n","            topi = topi[:3]\n","\n","        #print(\"Final Topi = {}\".format(topi))\n","        #print(\"input_ids = {}\".format(ids_to_decode))\n","        final_indexes = []\n","        count = 0\n","        count1 = 0\n","        #print(ids_to_decode[i], tokens_to_decode[i])\n","        #Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n","        while ids_to_decode[i][count] != 0:\n","            if count in topi:\n","                while ids_to_decode[i][count + count1 + 1] != 0:\n","                    if \"|\" in tokens_to_decode[i][count + count1 + 1]:\n","                        count1 += 1\n","                    else:\n","                        break\n","                count += count1\n","                count1 = 0\n","            else:\n","                final_indexes.append(ids_to_decode[i][count])\n","            count += 1\n","\n","        #print(final_indexes)\n","        temp_out_sen = tokenizer.decode(final_indexes,skip_special_token=True)\n","        #print(temp_out_sen, \"\\n\\n\")\n","        #temp_out_sen = \" \".join(temp_out_sen).replace(\"</s>\",\"\").replace(\"|\", \"\")\n","\n","        temp_out_sen = temp_out_sen.split(\"</s>\")[0]\n","        out_sen[i] = temp_out_sen.strip()\n","    \n","    return out_sen"]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(train_0_data, layer=6, head=0, batch_size=128)\n","train_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(train_0_data, train_0_out_sen, train_0_out, sentiment=\"<NEG>\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aw9cc04-Sxxx","outputId":"42f62dba-09a7-4e53-d01a-92768440fad2","executionInfo":{"status":"ok","timestamp":1682190994836,"user_tz":420,"elapsed":3497871,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 177218/177218 [00:22<00:00, 7936.95it/s]\n","100%|██████████| 1385/1385 [30:13<00:00,  1.31s/it]\n","100%|██████████| 177218/177218 [27:09<00:00, 108.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.train.0\n"]}]},{"cell_type":"code","source":["train_0_out_sen"],"metadata":{"id":"bEy11E7lrMOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(train_1_data, layer=6, head=0, batch_size=128)\n","train_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(train_1_data, train_1_out_sen, train_1_out, sentiment=\"<POS>\")"],"metadata":{"id":"0Cc3wViQS1eH","executionInfo":{"status":"ok","timestamp":1682196350267,"user_tz":420,"elapsed":5165175,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b13a16a4-5d6a-42d6-d858-e4251f11b936"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 266041/266041 [00:31<00:00, 8522.16it/s] \n","100%|██████████| 2079/2079 [45:35<00:00,  1.32s/it]\n","100%|██████████| 266041/266041 [39:15<00:00, 112.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.train.1\n"]}]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(dev_0_data, layer=6, head=0, batch_size=128)\n","dev_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(dev_0_data, dev_0_out_sen, dev_0_out, sentiment=\"<NEG>\")"],"metadata":{"id":"rch3daE1XN4x","executionInfo":{"status":"ok","timestamp":1682191100956,"user_tz":420,"elapsed":57968,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0fe4bc5-ed05-406f-faab-8d93f72603cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 2000/2000 [00:00<00:00, 3593.05it/s]\n","100%|██████████| 16/16 [00:22<00:00,  1.38s/it]\n","100%|██████████| 2000/2000 [00:34<00:00, 58.22it/s] "]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.dev.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(dev_1_data, layer=6, head=0, batch_size=128)\n","dev_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(dev_1_data, dev_1_out_sen, dev_1_out, sentiment=\"<POS>\")"],"metadata":{"id":"hjRmOd4JcXs2","executionInfo":{"status":"ok","timestamp":1682196387411,"user_tz":420,"elapsed":37045,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d67e2d0b-8f8e-415d-d75f-3badb343a1a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:00<00:00, 10557.68it/s]\n","100%|██████████| 16/16 [00:18<00:00,  1.17s/it]\n","100%|██████████| 2000/2000 [00:17<00:00, 112.62it/s]"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.dev.1\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(test_1_data, layer=6, head=0, batch_size=128)\n","test_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(test_1_data, test_1_out_sen, test_1_out, sentiment=\"<POS>\")"],"metadata":{"id":"LTGBbJwpccMQ","executionInfo":{"status":"ok","timestamp":1682196396951,"user_tz":420,"elapsed":9558,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9caf2d11-99b7-44e0-bf3e-0789a74753cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 5866.59it/s]\n","100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n","100%|██████████| 500/500 [00:04<00:00, 112.37it/s]"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.test.1\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(test_0_data, layer=6, head=0, batch_size=128)\n","test_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(test_0_data, test_0_out_sen, test_0_out, sentiment=\"<NEG>\")"],"metadata":{"id":"Vurlj2A3ci0H","executionInfo":{"status":"ok","timestamp":1682196407721,"user_tz":420,"elapsed":10792,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25a3c868-f8da-41d0-a012-5acffebb5d5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 9735.90it/s]\n","100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n","100%|██████████| 500/500 [00:05<00:00, 85.00it/s] "]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.test.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(ref_1_data, layer=6, head=0, batch_size=128)\n","ref_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_ref_output_file(ref_1_out_sen, reference_1_out,sentiment=\"<NEG>\")"],"metadata":{"id":"18g5oAKsct5O","executionInfo":{"status":"ok","timestamp":1682209795764,"user_tz":420,"elapsed":15976,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a6f7a3b-511a-48d3-ccfa-dcb3a50b0999"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 500/500 [00:00<00:00, 8353.66it/s]\n","100%|██████████| 4/4 [00:07<00:00,  1.93s/it]\n","100%|██████████| 500/500 [00:07<00:00, 63.05it/s]\n","100%|██████████| 500/500 [00:00<00:00, 690989.13it/s]\n"]}]},{"cell_type":"code","source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(ref_0_data, layer=6, head=0, batch_size=128)\n","ref_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_ref_output_file(ref_0_out_sen, reference_0_out, sentiment=\"<POS>\")"],"metadata":{"id":"c0sIefgBcvVU","executionInfo":{"status":"ok","timestamp":1682205244771,"user_tz":420,"elapsed":11473,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9719375-4312-4b6c-aca0-d2f163f09823"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 8269.10it/s]\n","100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n","100%|██████████| 500/500 [00:06<00:00, 73.35it/s] \n","100%|██████████| 500/500 [00:00<00:00, 673459.22it/s]\n"]}]},{"cell_type":"code","source":["ta = data2 = \"\"\n"," \n","# Reading data from file1\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.train.0') as fp:\n","    data = fp.read()\n"," \n","# Reading data from file2\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.train.1') as fp:\n","    data2 = fp.read()\n"," \n","# Merging 2 files\n","# To add the data of file2\n","# from next line\n","data += \"\\n\"\n","data += data2\n"," \n","with open ('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.train', 'w+') as fp:\n","    fp.write(data)"],"metadata":{"id":"JfvV9Ti_Tn1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ta = data2 = \"\"\n"," \n","# Reading data from file1\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.dev.0') as fp:\n","    data = fp.read()\n"," \n","# Reading data from file2\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.dev.1') as fp:\n","    data2 = fp.read()\n"," \n","# Merging 2 files\n","# To add the data of file2\n","# from next line\n","data += \"\\n\"\n","data += data2\n"," \n","with open ('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.dev', 'w+') as fp:\n","    fp.write(data)"],"metadata":{"id":"loYeUSnGT61L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ta = data2 = \"\"\n"," \n","# Reading data from file1\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.test.0') as fp:\n","    data = fp.read()\n"," \n","# Reading data from file2\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.test.1') as fp:\n","    data2 = fp.read()\n"," \n","# Merging 2 files\n","# To add the data of file2\n","# from next line\n","data += \"\\n\"\n","data += data2\n"," \n","with open ('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/sentiment.test', 'w+') as fp:\n","    fp.write(data)"],"metadata":{"id":"Ef0mRoJSUlEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ta = data2 = \"\"\n"," \n","# Reading data from file1\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/reference.0') as fp:\n","    data = fp.read()\n"," \n","# Reading data from file2\n","with open('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/reference.1') as fp:\n","    data2 = fp.read()\n"," \n","# Merging 2 files\n","# To add the data of file2\n","# from next line\n","data += \"\\n\"\n","data += data2\n"," \n","with open ('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/processed_files_with_t5_with_best_head/reference', 'w+') as fp:\n","    fp.write(data)"],"metadata":{"id":"z0NhgW4NUqam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uYkgKoROVfCz"},"execution_count":null,"outputs":[]}]}