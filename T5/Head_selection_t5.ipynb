{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"58f2f66c067e47e88b6393dc3814d86e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58a011bc859a48218cc282ae2f79abf0","IPY_MODEL_d167905814b04e379d5ff7990c13a434","IPY_MODEL_6202d2b610c24437b3a0c85bca96766c"],"layout":"IPY_MODEL_2ac3d4c39c6642899df9d93ce65981a6"}},"58a011bc859a48218cc282ae2f79abf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c5fe1f78966477399650d298ea76f47","placeholder":"​","style":"IPY_MODEL_7648fda395604a48b23876e9b57ec51b","value":"Downloading (…)ve/main/spiece.model: 100%"}},"d167905814b04e379d5ff7990c13a434":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79787c882cd84f3581b61ac81ab32568","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bbacd3a90304439a1c2913ed69fdc12","value":791656}},"6202d2b610c24437b3a0c85bca96766c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b2c394630a1488a9920fae0d0334816","placeholder":"​","style":"IPY_MODEL_bb3a3faf135c4684948daf7eb3749a55","value":" 792k/792k [00:00&lt;00:00, 6.38MB/s]"}},"2ac3d4c39c6642899df9d93ce65981a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c5fe1f78966477399650d298ea76f47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7648fda395604a48b23876e9b57ec51b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79787c882cd84f3581b61ac81ab32568":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bbacd3a90304439a1c2913ed69fdc12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b2c394630a1488a9920fae0d0334816":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb3a3faf135c4684948daf7eb3749a55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4a6ce3c6da94d718dd10eaff3914ece":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce8af1faa3764bafb74525499190b4af","IPY_MODEL_90a721b0689244dc9a73339db0ed5a76","IPY_MODEL_07549e7893d1421bb43f4eb465aee3e2"],"layout":"IPY_MODEL_8be65b7f62ae4a658e36eb4343ef83f8"}},"ce8af1faa3764bafb74525499190b4af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75d532312ae54e4fb554dc9239a24c85","placeholder":"​","style":"IPY_MODEL_5cca930a27784c3d9349fa1a3533b447","value":"Downloading (…)lve/main/config.json: 100%"}},"90a721b0689244dc9a73339db0ed5a76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d1bcfbca0a4661ad92abdd1b202454","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b38212c80a8b44a68a756ef07986ece0","value":1208}},"07549e7893d1421bb43f4eb465aee3e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_070e8dae0a6e4485bfe9e6c6930eeb97","placeholder":"​","style":"IPY_MODEL_d75718fb885b49e79faefed0bd9818b1","value":" 1.21k/1.21k [00:00&lt;00:00, 13.7kB/s]"}},"8be65b7f62ae4a658e36eb4343ef83f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75d532312ae54e4fb554dc9239a24c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cca930a27784c3d9349fa1a3533b447":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52d1bcfbca0a4661ad92abdd1b202454":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b38212c80a8b44a68a756ef07986ece0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"070e8dae0a6e4485bfe9e6c6930eeb97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d75718fb885b49e79faefed0bd9818b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98379d2daaeb4349b621e99341e0e5e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_206d16a37bd8417aa76ef6e598cfacf4","IPY_MODEL_9ff34dc151134d19840be4ca879fe2b4","IPY_MODEL_77449072e2e94bbc8431d0a8b6c7a418"],"layout":"IPY_MODEL_7a0f688f37134a0ca0ba178278e21e6a"}},"206d16a37bd8417aa76ef6e598cfacf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d820e35f678543948e374261b7b618b0","placeholder":"​","style":"IPY_MODEL_bc5150e474e54003b9806e4f772b57dc","value":"100%"}},"9ff34dc151134d19840be4ca879fe2b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dbded087d794fd58485190895d05d3e","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd54a1578f574f0d94e625aa9bf31714","value":12}},"77449072e2e94bbc8431d0a8b6c7a418":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5336070668ed45cda5c7a350a35aee58","placeholder":"​","style":"IPY_MODEL_660f9714ee494e7e8fd1f88510eac1bb","value":" 12/12 [19:22&lt;00:00, 96.00s/it]"}},"7a0f688f37134a0ca0ba178278e21e6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d820e35f678543948e374261b7b618b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc5150e474e54003b9806e4f772b57dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dbded087d794fd58485190895d05d3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd54a1578f574f0d94e625aa9bf31714":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5336070668ed45cda5c7a350a35aee58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"660f9714ee494e7e8fd1f88510eac1bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2756e638670247f389b0338ab2bb99bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_924cd52155b547fd97ce2b9d9025dba5","IPY_MODEL_093b14202ed047e997742c21fa03e7ed","IPY_MODEL_da53a5cc26184b5c9a15a94af4bbdae7"],"layout":"IPY_MODEL_69a95b7c9b634f0cab2ede6d38413e55"}},"924cd52155b547fd97ce2b9d9025dba5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e16e32c56c364883b342b1f01602dc91","placeholder":"​","style":"IPY_MODEL_e825dbec050242649d93a1aafa183980","value":"100%"}},"093b14202ed047e997742c21fa03e7ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac0a5e158f24afe83c20cde7ee127e6","max":144,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28e0c1a060ea4081b5521a22c0fee1c3","value":144}},"da53a5cc26184b5c9a15a94af4bbdae7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04220e3148314ae0b69b75e940faaf83","placeholder":"​","style":"IPY_MODEL_c2e8d138cb594995bbb6aa320ecfa22b","value":" 144/144 [10:26&lt;00:00,  4.34s/it]"}},"69a95b7c9b634f0cab2ede6d38413e55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e16e32c56c364883b342b1f01602dc91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e825dbec050242649d93a1aafa183980":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fac0a5e158f24afe83c20cde7ee127e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28e0c1a060ea4081b5521a22c0fee1c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04220e3148314ae0b69b75e940faaf83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2e8d138cb594995bbb6aa320ecfa22b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaGVAb3bsxdo","executionInfo":{"status":"ok","timestamp":1682285803639,"user_tz":420,"elapsed":23456,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"27ba4bc2-0df1-4b69-cdc7-b8a26a3c4d7b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! pip install SentencePiece==0.1.94"],"metadata":{"id":"K42CSBxc3T1U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682285823452,"user_tz":420,"elapsed":5129,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"61a80fa6-da67-4bfe-9643-4064bb45d8f4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SentencePiece==0.1.94\n","  Downloading sentencepiece-0.1.94-cp39-cp39-manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: SentencePiece\n","Successfully installed SentencePiece-0.1.94\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"eXLm-HJx2h-5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682285847839,"user_tz":420,"elapsed":11190,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"a92a794d-970e-41e7-b0aa-c056b3cb2f60"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"code","source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkJFSKMv93wi","executionInfo":{"status":"ok","timestamp":1682285861879,"user_tz":420,"elapsed":14060,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"836bfd6a-b4f2-4853-84f2-0d1eee4507fa"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"P8FrNclAJHey","executionInfo":{"status":"ok","timestamp":1682272821301,"user_tz":420,"elapsed":5,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/transformer-drg-style-transfer-master/"],"metadata":{"id":"FmVQJFsm2adA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682285885122,"user_tz":420,"elapsed":141,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"15da1383-cabc-450f-f0a2-44cff70dd7fa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1gvgdEyQQFFN43xnL2_DdI4rUtMI5gnmU/transformer-drg-style-transfer-master\n"]}]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration"],"metadata":{"id":"1zcHeYBU3QvE","executionInfo":{"status":"ok","timestamp":1682285886998,"user_tz":420,"elapsed":98,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model_cls = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment')\n","# #model_cls.load_state_dict(torch.load('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/pytorch_model.bin'))\n","# model_cls.eval()"],"metadata":{"id":"jsDWgumX1fSm","executionInfo":{"status":"ok","timestamp":1682285888537,"user_tz":420,"elapsed":115,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer = T5Tokenizer.from_pretrained('t5-base', max_length=70)\n","model =  T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment', return_dict = True)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/pytorch_model.bin', map_location=torch.device(device)))\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["58f2f66c067e47e88b6393dc3814d86e","58a011bc859a48218cc282ae2f79abf0","d167905814b04e379d5ff7990c13a434","6202d2b610c24437b3a0c85bca96766c","2ac3d4c39c6642899df9d93ce65981a6","6c5fe1f78966477399650d298ea76f47","7648fda395604a48b23876e9b57ec51b","79787c882cd84f3581b61ac81ab32568","2bbacd3a90304439a1c2913ed69fdc12","0b2c394630a1488a9920fae0d0334816","bb3a3faf135c4684948daf7eb3749a55","f4a6ce3c6da94d718dd10eaff3914ece","ce8af1faa3764bafb74525499190b4af","90a721b0689244dc9a73339db0ed5a76","07549e7893d1421bb43f4eb465aee3e2","8be65b7f62ae4a658e36eb4343ef83f8","75d532312ae54e4fb554dc9239a24c85","5cca930a27784c3d9349fa1a3533b447","52d1bcfbca0a4661ad92abdd1b202454","b38212c80a8b44a68a756ef07986ece0","070e8dae0a6e4485bfe9e6c6930eeb97","d75718fb885b49e79faefed0bd9818b1"]},"id":"Kfpfk7ftDiX1","executionInfo":{"status":"ok","timestamp":1682285916843,"user_tz":420,"elapsed":24843,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"9ad49978-03b1-4ded-ae57-f4d19d90ac3f"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f2f66c067e47e88b6393dc3814d86e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a6ce3c6da94d718dd10eaff3914ece"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["tokenizer = T5Tokenizer.from_pretrained('t5-base', max_length=70)\n","model =  T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment', return_dict = True)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/transformer-drg-style-transfer-master/t5_base_yelp_sentiment/pytorch_model.bin', map_location=torch.device(device)))\n","model.eval()"],"metadata":{"id":"J1D-9W-h4OdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.auto import tqdm"],"metadata":{"id":"_WzzCjPVBVeJ","executionInfo":{"status":"ok","timestamp":1682285937535,"user_tz":420,"elapsed":111,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class YelpDataset(Dataset):\n","  def __init__(self, tokenizer, data_dir, type_path,  max_len=512, truncation = True):\n","    self.file_path = '/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/t5/dev.csv'\n","    \n","    self.truncation = truncation\n","    \n","    self.max_len = max_len\n","    self.tokenizer = tokenizer\n","    self.inputs = []\n","    self.targets = []\n","\n","    self._build()\n","  \n","  def __len__(self):\n","    return len(self.inputs)\n","  \n","  def __getitem__(self, index):\n","    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","    target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n","  \n","  def _build(self):\n","    self._buil_examples_from_files(self.file_path)\n","  \n","  def _buil_examples_from_files(self, files):\n","    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()]\")\n","    REPLACE_WITH_SPACE = re.compile(\"()|(\\-)|(\\/)\")\n","    df = pd.read_csv('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/t5/dev.csv', on_bad_lines='skip', delimiter='\\t', header=None, index_col=None)\n","    df.dropna(inplace=True)\n","    df = df.groupby(df[1]).sample(n=500, random_state = 42)\n","    df[1] = df[1].apply(str)\n","    # df[0] = df[0].apply(lambda text: REPLACE_NO_SPACE.sub(\"\", text))\n","    # X = df[0].apply(lambda text : REPLACE_WITH_SPACE.sub(\"\", text))\n","    X = df[0]\n","    y = df[1]\n","    \n","    \n","    for index, row in tqdm(enumerate(X)):\n","      # tokenize inputs\n","      tokenized_inputs = self.tokenizer.batch_encode_plus([X.iloc[index]], max_length=self.max_len, \n","                                                          pad_to_max_length=True, \n","                                                          return_tensors=\"pt\", truncation = self.truncation)\n","      tokenized_targets = self.tokenizer.batch_encode_plus([y.iloc[index]], \n","                                                           max_length=2, \n","                                                           pad_to_max_length=True, \n","                                                           return_tensors=\"pt\", truncation = self.truncation)\n","\n","\n","       # tokenize targets\n","    \n","      self.inputs.append(tokenized_inputs)\n","      self.targets.append(tokenized_targets)\n","     "],"metadata":{"id":"kXBux8K-9cY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjLqyVAwDaVM","executionInfo":{"status":"ok","timestamp":1682220870121,"user_tz":420,"elapsed":8,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"3f6dc2eb-a593-4826-dcd5-39e3afa7a16a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Apr 23 03:34:29 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P0    32W /  70W |   1805MiB / 15360MiB |     32%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["model.cuda()\n","# model_cls.cuda()"],"metadata":{"id":"r9t4W9IIEtf3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682220870767,"user_tz":420,"elapsed":650,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"05975a0a-b71a-49c8-f44b-0b14c7fa4c01"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Attentions"],"metadata":{"id":"FIeunx8kNCHv"}},{"cell_type":"code","source":["def get_attention_for_encoded_dataset(dataset, batch_size=10):\n","    \"\"\"\n","    returns attention weights in the form of (num_layers, total_data, num_heads, sequence_length, sequence_length)\n","    \"\"\"\n","    all_attn_probs = []\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    it = iter(loader)\n","    model.to(device)\n","\n","    with torch.no_grad():\n","        for batch in tqdm(it):\n","            batch_attn_probs = []  # To store attention weights for each layer in the current batch\n","            torch.cuda.empty_cache()\n","            input_ids = batch.to(device)\n","            outputs = model(input_ids=input_ids, output_attentions=True, decoder_input_ids=input_ids)\n","            attn = outputs.cross_attentions\n","\n","            # Unbatch the attention weights for each layer\n","            for layer in attn:\n","                batch_attn_probs.append(layer.detach().cpu().unsqueeze(1))\n","            batch_attn_probs = torch.cat(batch_attn_probs, dim=1)  # Shape: (batch_size, num_heads, sequence_length, sequence_length)\n","            all_attn_probs.append(batch_attn_probs)\n","\n","    # Stack the attention weights across all batches\n","    all_attn_probs = torch.cat(all_attn_probs, dim=0)  # Shape: (total_data, num_heads, sequence_length, sequence_length)\n","\n","    # Transpose to get the required shape\n","    # Shape: (num_layers, sequence_length, total_data, num_heads, sequence_length)\n","\n","    all_attn_probs = all_attn_probs.transpose(0, 1).transpose(1, 2)  # Shape: (num_layers, num_heads, total_data, sequence_length, sequence_length)\n","    \n","    return all_attn_probs\n"],"metadata":{"id":"51JVHQuF5SoO","executionInfo":{"status":"ok","timestamp":1682285916843,"user_tz":420,"elapsed":3,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# dataset = YelpDataset(tokenizer, 'aclImdb', 'test',  max_len=72)\n","\n","df = pd.read_csv('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/t5/dev.csv', on_bad_lines='skip', delimiter='\\t', header=None, index_col=None)\n","df.dropna(inplace=True)\n","df = df.groupby(df[1]).sample(n=200, random_state = 42)\n","df[1] = df[1].apply(str)\n","X = df[0].values.tolist()\n","\n","\n","input = tokenizer(X, max_length=72, pad_to_max_length=True, \n","                                                          return_tensors=\"pt\", truncation = True)\n","input_ids = input['input_ids'] \n","\n","attn =  get_attention_for_encoded_dataset(input_ids, batch_size=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"-54jHsIGLrCO","outputId":"04b42094-ff72-4f75-d4a9-9ab01d591935","executionInfo":{"status":"error","timestamp":1682285919040,"user_tz":420,"elapsed":2199,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-157c91c50c03>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_attention_for_encoded_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-46858a6f2e5c>\u001b[0m in \u001b[0;36mget_attention_for_encoded_dataset\u001b[0;34m(dataset, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mbatch_attn_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# To store attention weights for each layer in the current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"]}]},{"cell_type":"code","source":["attn = torch.load('attn.pt')"],"metadata":{"id":"kdTWtmgW8Kpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attn.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPFVhuxjiFhB","executionInfo":{"status":"ok","timestamp":1681931085104,"user_tz":420,"elapsed":105,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"bc25f955-cc67-4f08-a368-35d4a63261bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12, 12, 200, 72, 72])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["torch.save(attn, 'attn.pt')"],"metadata":{"id":"hAJAlTsCWoMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import psutil\n","\n","def clear_ram_cache():\n","    \"\"\"\n","    Clears the RAM cache on a Linux-based system.\n","    \"\"\"\n","    os.system(\"sync; echo 1 > /proc/sys/vm/drop_caches\")\n","    print(\"RAM cache cleared.\")\n","\n","clear_ram_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whfRdNF5WYPr","executionInfo":{"status":"ok","timestamp":1682220888953,"user_tz":420,"elapsed":366,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"17c12fd0-8398-449f-fbdb-7619beeee0f7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["RAM cache cleared.\n"]}]},{"cell_type":"markdown","source":["# removal of top k tokens"],"metadata":{"id":"84I_J8PoZV0t"}},{"cell_type":"code","source":["def process_sentences(input_sentences, att, remove_special_tokens=True, threshold=0.06):\n","    \"\"\"\n","    This function processes each input sentence by removing the top tokens defined threshold value.\n","    Each sentence is processed for each head.\n","\n","    Args:\n","        input_sentences (list): List of input sentences\n","        decoding_ids (list): List of encoded input vectors\n","        att (torch.Tensor): T5 attention tensor of shape (num_layers, num_heads, total_size, seq_length, seq_length)\n","        remove_special_tokens (bool): Whether to remove special tokens from the output (default: True)\n","        threshold (float): Percentage of the top indexes to be removed (default: 0.1)\n","\n","    Returns:\n","        List of processed sentences, where each sentence is a list of processed strings for each head.\n","    \"\"\"\n","    num_layers, num_heads, total_size, seq_length, _ = att.size()\n","\n","    # Convert the attention tensor to a NumPy array\n","    attention_np = att.cpu().detach().numpy()\n","\n","    # Create an empty list to store the processed sentences\n","    processed_sentences = [None for j in range((num_heads)*(num_layers))]\n","\n","    inx = 0\n","    for i in tqdm(range(num_layers)):\n","        for j in range(num_heads):\n","            processed_head = [None for k in range(total_size)]\n","            for k in range(total_size):\n","                # Get the encoded input vector for the kth input sentence\n","                encoded_input_k = input_sentences[k]\n","\n","                # Get the attention scores for the kth input sentence and jth head\n","                att_scores = attention_np[i, j, k]\n","                \n","\n","                # Get the top indexes to be removed\n","                topk = int(len(att_scores[0]) * threshold)\n","                topi = att_scores[0].argsort()[::-1][:topk]\n","\n","\n","                # Remove the top indexes from the encoded input vector\n","                # print('------------')\n","                # print(encoded_input_k)\n","                \n","                encoded_input_trimmed = [token_id for idx, token_id in enumerate(encoded_input_k) if idx not in topi]\n","                #print(encoded_input_trimmed)\n","\n","                # Convert the encoded input vector back to a string\n","                decoded_str = tokenizer.decode(encoded_input_trimmed, skip_special_tokens=True)\n","                #print(decoded_str)\n","                \n","\n","                processed_head[k] = decoded_str\n","            # print('++++++++++++')\n","            # print(processed_head[:10])\n","            processed_sentences[inx] = processed_head\n","            inx+=1\n","      \n","\n","    return processed_sentences\n"],"metadata":{"id":"X37VGZLXZUxH","executionInfo":{"status":"ok","timestamp":1682220892786,"user_tz":420,"elapsed":634,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# df = pd.read_csv('/content/drive/MyDrive/transformer-drg-style-transfer-master/data/yelp/t5/dev.csv', on_bad_lines='skip', delimiter='\\t', header=None, index_col=None)\n","# df.dropna(inplace=True)\n","# df = df.groupby(df[1]).sample(n=500, random_state = 42)\n","# df[1] = df[1].apply(str)\n","# X = df[0].values.tolist()\n","\n","\n","# input = tokenizer(X, max_length=72, pad_to_max_length=True, \n","#                                                           return_tensors=\"pt\", truncation = True)\n","# input_ids = input['input_ids'] \n","\n","sen_list = process_sentences(input_ids, attn)"],"metadata":{"id":"M1zmbLV6-OTw","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["98379d2daaeb4349b621e99341e0e5e8","206d16a37bd8417aa76ef6e598cfacf4","9ff34dc151134d19840be4ca879fe2b4","77449072e2e94bbc8431d0a8b6c7a418","7a0f688f37134a0ca0ba178278e21e6a","d820e35f678543948e374261b7b618b0","bc5150e474e54003b9806e4f772b57dc","5dbded087d794fd58485190895d05d3e","dd54a1578f574f0d94e625aa9bf31714","5336070668ed45cda5c7a350a35aee58","660f9714ee494e7e8fd1f88510eac1bb"]},"executionInfo":{"status":"ok","timestamp":1682222056633,"user_tz":420,"elapsed":1161756,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"3b12d8cf-53b0-452b-e8cb-af6234ec4a9d"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98379d2daaeb4349b621e99341e0e5e8"}},"metadata":{}}]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"test\", \"wb\") as fp:   #Pickling\n","  pickle.dump(sen_list, fp)"],"metadata":{"id":"TtXtOwT5CAVi","executionInfo":{"status":"ok","timestamp":1682222066603,"user_tz":420,"elapsed":996,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open('test', 'rb') as f:\n","    sen_list = pickle.load(f)"],"metadata":{"id":"vHp_qlc6uinW","executionInfo":{"status":"ok","timestamp":1682285923650,"user_tz":420,"elapsed":577,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def process_multiple_sentences(sen_list, batch_size=10):\n","    #model_cls.cuda()\n","    model_inputs = []\n","    pred_lt = []\n","\n","\n","    input = tokenizer(sen_list, max_length=72, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n","    input_ids = input['input_ids']\n","\n","    loader = DataLoader(input_ids, batch_size=batch_size, shuffle=False)\n","    it = iter(loader)\n","    model.to(device)\n","    class_scores = np.zeros((len(sen_list), 2))\n","    idx = 0\n","    with torch.no_grad():\n","        for batch in it:\n","            torch.cuda.empty_cache()\n","            batch = batch.to(device)\n","            outputs = model(input_ids=batch, decoder_input_ids=batch)\n","            logits = outputs.logits[:, 1]\n","            probs = torch.softmax(logits, dim=-1)\n","            scores = probs[:, 1]\n","            scores_0 = (1 - scores)\n","            scores = scores.cpu().numpy()\n","            scores_0 = scores_0.cpu().numpy()\n","            \n","            temp_list = np.column_stack((scores_0, scores))\n","            #print(class_scores.shape, scores.shape, scores_0.shape, temp_list.shape)\n","            class_scores[idx:idx+batch_size] = temp_list\n","            idx+=batch_size\n","    \n","    \n","    # Obtain the classification probability of the predicted output\n","\n","    #print(class_scores)\n","    return class_scores\n"],"metadata":{"id":"YL15Ne0s7v7p","executionInfo":{"status":"ok","timestamp":1682285927276,"user_tz":420,"elapsed":4,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def get_block_head(processed_sentence_list, lmbd = 0.1):\n","    \"\"\"\n","    This function calculate classification scores for sentences generated by each head\n","    and sort them from best to worst.\n","    score = min(pred) + lmbd / max(pred) + lmbd, lmbd is smoothing param\n","    pred is list of probability score for each class, for best case pred = [0.5, 0.5] ==> score = 1\n","    \n","    it returns sorted list of (Layer, Head, Score)\n","    \"\"\"\n","    scores = {}\n","    #scores_1 = {}\n","    for i in tqdm(range(len(processed_sentence_list))): # sentences by each head\n","        #print(processed_sentence_list[i])\n","        pred = np.array(process_multiple_sentences(processed_sentence_list[i]))\n","        scores[i] = np.mean([(min(x[0], x[1])+lmbd)/(max(x[0], x[1])+lmbd) for x in pred])\n","        #scores_1[i] = np.mean([abs(max(x[0],x[1]) - min(x[0],x[1])) for x in pred])\n","    temp = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n","    #temp1 = sorted(scores_1.items(), key=lambda kv: kv[1], reverse=False)\n","    score_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp]\n","    #score1_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp1]\n","    return score_lt  #score1_lt"],"metadata":{"id":"qNO_oC-PHqkT","executionInfo":{"status":"ok","timestamp":1682285927651,"user_tz":420,"elapsed":1,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["scores = get_block_head(sen_list)"],"metadata":{"id":"5Sh3lZV7PZ0q","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["2756e638670247f389b0338ab2bb99bf","924cd52155b547fd97ce2b9d9025dba5","093b14202ed047e997742c21fa03e7ed","da53a5cc26184b5c9a15a94af4bbdae7","69a95b7c9b634f0cab2ede6d38413e55","e16e32c56c364883b342b1f01602dc91","e825dbec050242649d93a1aafa183980","fac0a5e158f24afe83c20cde7ee127e6","28e0c1a060ea4081b5521a22c0fee1c3","04220e3148314ae0b69b75e940faaf83","c2e8d138cb594995bbb6aa320ecfa22b"]},"executionInfo":{"status":"ok","timestamp":1682286569590,"user_tz":420,"elapsed":626644,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"09cf84ec-7213-4384-9d3a-b7a0f8d0a09c"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/144 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2756e638670247f389b0338ab2bb99bf"}},"metadata":{}}]},{"cell_type":"code","source":["scores"],"metadata":{"id":"Rml5-AgswSOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682286571542,"user_tz":420,"elapsed":138,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"efa9d176-bdc6-44ad-f19f-e5214c58ad6d"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(6, 0, 0.1886666047780669),\n"," (8, 3, 0.17818111172669035),\n"," (6, 11, 0.17782135217180708),\n"," (6, 6, 0.1768796998756291),\n"," (11, 3, 0.1765833478632518),\n"," (1, 7, 0.17653317507348854),\n"," (8, 10, 0.1745621665480632),\n"," (5, 0, 0.1740807017821691),\n"," (4, 0, 0.17360569234366177),\n"," (10, 10, 0.17328406888563258),\n"," (6, 9, 0.17286359385375788),\n"," (6, 7, 0.17272362104082264),\n"," (2, 7, 0.17172294613856137),\n"," (1, 11, 0.1716852533925048),\n"," (11, 9, 0.17160905469217647),\n"," (6, 1, 0.17099513453388185),\n"," (5, 10, 0.1702280725273276),\n"," (7, 7, 0.1695993064579305),\n"," (11, 10, 0.16950135331922805),\n"," (4, 1, 0.1692915988612422),\n"," (9, 8, 0.16895496215591002),\n"," (9, 2, 0.1684952110633918),\n"," (11, 7, 0.1684192828801784),\n"," (7, 11, 0.16823850942629967),\n"," (4, 9, 0.1680505133021179),\n"," (5, 4, 0.16782368293738795),\n"," (3, 6, 0.16750841246488563),\n"," (5, 7, 0.16748404254579563),\n"," (7, 10, 0.16703825371470432),\n"," (11, 2, 0.16696967743442365),\n"," (10, 0, 0.16690263155451732),\n"," (7, 8, 0.16689493191398747),\n"," (1, 4, 0.16687444802620227),\n"," (1, 3, 0.16663370540641695),\n"," (9, 11, 0.16657348160322294),\n"," (7, 1, 0.1664660375686742),\n"," (10, 9, 0.16591064158661614),\n"," (8, 8, 0.16582716898774),\n"," (9, 4, 0.165502816688645),\n"," (11, 1, 0.165479296235468),\n"," (8, 2, 0.16541918534216415),\n"," (3, 3, 0.16513344700243723),\n"," (6, 5, 0.16469166553930203),\n"," (7, 2, 0.164650337004505),\n"," (7, 4, 0.16457950019385684),\n"," (1, 10, 0.16400944541661538),\n"," (7, 9, 0.16388772973780008),\n"," (2, 0, 0.16360314191149478),\n"," (3, 7, 0.16347877348166126),\n"," (8, 7, 0.16331793320390744),\n"," (8, 0, 0.1632061063588268),\n"," (10, 8, 0.16289906335592833),\n"," (2, 1, 0.16266154734623925),\n"," (6, 8, 0.16257109531892927),\n"," (10, 3, 0.16212561397133013),\n"," (7, 3, 0.1621252701283168),\n"," (0, 7, 0.16209673133681335),\n"," (4, 3, 0.16183612465909922),\n"," (3, 4, 0.16181012239866796),\n"," (3, 10, 0.1618029189487479),\n"," (11, 8, 0.16161876370586184),\n"," (7, 6, 0.1615758671827269),\n"," (10, 1, 0.1614505251151684),\n"," (11, 5, 0.16145009325885035),\n"," (9, 9, 0.1614429116741759),\n"," (3, 8, 0.16095162034141194),\n"," (9, 7, 0.16088651803366638),\n"," (5, 11, 0.16069385412064519),\n"," (6, 3, 0.160570270742706),\n"," (6, 2, 0.16054290118837816),\n"," (5, 3, 0.16004458750534764),\n"," (8, 4, 0.15980179093172062),\n"," (6, 4, 0.15899127763707502),\n"," (2, 6, 0.1583717080197045),\n"," (3, 11, 0.15827605472338338),\n"," (3, 1, 0.15764285135168588),\n"," (8, 11, 0.1574411162973074),\n"," (5, 8, 0.15697328230863136),\n"," (10, 4, 0.15655273544822024),\n"," (5, 2, 0.15652104278336565),\n"," (5, 6, 0.1564687094584729),\n"," (10, 5, 0.1564682636511244),\n"," (5, 1, 0.1564311236952469),\n"," (5, 5, 0.15631239140486763),\n"," (9, 3, 0.1561702671537767),\n"," (10, 7, 0.15614388163959853),\n"," (4, 7, 0.155322358646674),\n"," (0, 8, 0.15469595401560274),\n"," (8, 5, 0.15467405264121153),\n"," (2, 9, 0.15422238101058625),\n"," (9, 1, 0.15405268909951222),\n"," (7, 0, 0.15374871199968235),\n"," (0, 5, 0.15350115803206232),\n"," (1, 6, 0.15319140440959256),\n"," (1, 2, 0.15313140583561113),\n"," (10, 11, 0.15290303899396487),\n"," (0, 1, 0.152666846558879),\n"," (11, 4, 0.15250165620903353),\n"," (4, 11, 0.15249483755269677),\n"," (2, 2, 0.15235105422840267),\n"," (0, 3, 0.1522386745251335),\n"," (2, 10, 0.15168020247747704),\n"," (6, 10, 0.15163932204212288),\n"," (4, 8, 0.15160659506513008),\n"," (1, 9, 0.15108752332227493),\n"," (8, 9, 0.15101689756272102),\n"," (8, 6, 0.15067021905676709),\n"," (0, 0, 0.15057311970415285),\n"," (4, 2, 0.15054434373313388),\n"," (0, 4, 0.1504968971221589),\n"," (2, 3, 0.14996670138847967),\n"," (11, 0, 0.14965158859305563),\n"," (9, 5, 0.1495674291543448),\n"," (2, 5, 0.14935025230145224),\n"," (1, 5, 0.14924641721322543),\n"," (10, 2, 0.14904624046382284),\n"," (4, 10, 0.14882118453100496),\n"," (9, 10, 0.14859743243611198),\n"," (4, 6, 0.14785892235748593),\n"," (3, 5, 0.14765934828906657),\n"," (1, 1, 0.14724559930120448),\n"," (8, 1, 0.14721333572794623),\n"," (9, 0, 0.14631344088732498),\n"," (11, 6, 0.14608007707129014),\n"," (4, 4, 0.14547614616760873),\n"," (3, 0, 0.1447764446365984),\n"," (3, 2, 0.14428443157090898),\n"," (7, 5, 0.1441700832091774),\n"," (4, 5, 0.1440546608372781),\n"," (5, 9, 0.14350591917746272),\n"," (3, 9, 0.14337227911071526),\n"," (0, 10, 0.14276213042487346),\n"," (1, 8, 0.1423455488740491),\n"," (11, 11, 0.1393549707107413),\n"," (2, 4, 0.1390153489432734),\n"," (2, 8, 0.13863211578582024),\n"," (9, 6, 0.13858106136001996),\n"," (0, 6, 0.1368584885674415),\n"," (0, 11, 0.1347001033958885),\n"," (0, 2, 0.13468687840264615),\n"," (10, 6, 0.1334383534950235),\n"," (1, 0, 0.1321074806325714),\n"," (0, 9, 0.13134972336804393),\n"," (2, 11, 0.12139750916306216)]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["scores"],"metadata":{"id":"E-h-8zelliwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = sen_list[0]\n","b = sen_list[1]\n","count = 0\n","for i in range(len(sen_list[0])):\n","  if a[i] != b[i]:\n","    print(a[i], b[i], count)\n","    count += 1"],"metadata":{"id":"3uVEZvtgHrRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3C6Fp2aPCMuz"},"execution_count":null,"outputs":[]}]}