{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fR1RSJad7Xmb","executionInfo":{"status":"ok","timestamp":1682472512373,"user_tz":420,"elapsed":2092,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"ecc7e975-0e21-4281-a3aa-d0e954c98527"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install torch"],"metadata":{"id":"Xmfi5cY8-DnG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472516601,"user_tz":420,"elapsed":4231,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"eb3309e4-ab06-4e09-9bce-3af77c613925"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["! pip install pytorch_pretrained_bert==0.6.1"],"metadata":{"id":"AYZMvnHO7cHF","executionInfo":{"status":"ok","timestamp":1682472521505,"user_tz":420,"elapsed":4914,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"735dbe92-1b6d-4b2d-bb63-bb0fb822999f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_pretrained_bert==0.6.1 in /usr/local/lib/python3.9/dist-packages (0.6.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (1.26.120)\n","Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (2.27.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (1.22.4)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (2.0.0+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (4.65.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (4.5.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (3.11.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (16.0.1)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.120 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch_pretrained_bert==0.6.1) (1.29.120)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch_pretrained_bert==0.6.1) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch_pretrained_bert==0.6.1) (0.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert==0.6.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert==0.6.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert==0.6.1) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch_pretrained_bert==0.6.1) (1.26.15)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.120->boto3->pytorch_pretrained_bert==0.6.1) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert==0.6.1) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.120->boto3->pytorch_pretrained_bert==0.6.1) (1.16.0)\n"]}]},{"cell_type":"code","source":["!pip install boto3"],"metadata":{"id":"de5zVFeV8Ijz","executionInfo":{"status":"ok","timestamp":1682472525677,"user_tz":420,"elapsed":4181,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0bb82389-0e3c-4e19-94d3-84db7af47be2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (1.26.120)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3) (0.6.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.120 in /usr/local/lib/python3.9/dist-packages (from boto3) (1.29.120)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.120->boto3) (1.26.15)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.120->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.120->boto3) (1.16.0)\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/riya\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6u3n5el-8U5","executionInfo":{"status":"ok","timestamp":1682472525678,"user_tz":420,"elapsed":4,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"042636c3-76fd-4d38-a5d4-111affac6571"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1TCvtUjcTYKWgalUeZbJk_aFhLp1aLjr6/riya\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"R74KkJhicv1R","executionInfo":{"status":"ok","timestamp":1682472531175,"user_tz":420,"elapsed":5500,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95840489-91a5-43af-bbae-14a5273591a0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aa1SOEG66naK","executionInfo":{"status":"ok","timestamp":1682472536042,"user_tz":420,"elapsed":4874,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"4552f8b5-c141-4ed1-8e4b-6ac5038eb489"},"outputs":[{"output_type":"stream","name":"stdout","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"]}],"source":["import csv\n","import logging\n","import os\n","import random\n","import sys\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n","from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n","#from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n","\n","from bertviz.bertviz import attention, visualization\n","from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"]},{"cell_type":"code","source":["from transformers import (\n","    BertForSequenceClassification,\n","    BertTokenizer,\n","    RobertaModel,\n","    RobertaForSequenceClassification,\n","    RobertaTokenizer,\n","    AdamW)"],"metadata":{"id":"DteNteR2dXJ4","executionInfo":{"status":"ok","timestamp":1682472540848,"user_tz":420,"elapsed":4813,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"yZ5YGqFB6naM","executionInfo":{"status":"ok","timestamp":1682472542615,"user_tz":420,"elapsed":1772,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["logger = logging.getLogger(__name__)\n","bert_classifier_model_dir = \"/content/drive/MyDrive/riya/robertaOutput/roberta.pt\" ## Path of BERT classifier model path\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"RTiLzVUQ6naN","executionInfo":{"status":"ok","timestamp":1682472542615,"user_tz":420,"elapsed":2,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["# file paths\n","data_dir = \"/content/drive/MyDrive/riya/data\"\n","dataset = \"yelp\" # amazon / yelp / imagecaption\n","train_0 = os.path.join(data_dir ,\"{}/sentiment.train.0\".format(dataset))\n","train_1 = os.path.join(data_dir,\"{}/sentiment.train.1\".format(dataset))\n","test_0 = os.path.join(data_dir,\"{}/sentiment.test.0\".format(dataset))\n","test_1 = os.path.join(data_dir,\"{}/sentiment.test.1\".format(dataset))\n","dev_0 = os.path.join(data_dir,\"{}/sentiment.dev.0\".format(dataset))\n","dev_1 = os.path.join(data_dir,\"{}/sentiment.dev.1\".format(dataset))\n","reference_0 = os.path.join(data_dir,\"{}/reference.0\".format(dataset))\n","reference_1 = os.path.join(data_dir,\"{}/reference.1\".format(dataset))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"aM29zQlN6naN","executionInfo":{"status":"ok","timestamp":1682472542615,"user_tz":420,"elapsed":2,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["# file paths\n","data_dir = \"/content/drive/MyDrive/riya/data\"\n","dataset = \"yelp\" # amazon / yelp / imagecaption\n","train_0_out = os.path.join(data_dir ,\"{}/processed_files_with_bert_with_best_head/sentiment_train_0.txt\".format(dataset))\n","train_1_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/sentiment_train_1.txt\".format(dataset))\n","test_0_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/sentiment_test_0.txt\".format(dataset))\n","test_1_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/sentiment_test_1.txt\".format(dataset))\n","dev_0_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/sentiment_dev_0.txt\".format(dataset))\n","dev_1_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/sentiment_dev_1.txt\".format(dataset))\n","reference_0_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/reference_0.txt\".format(dataset))\n","reference_1_out = os.path.join(data_dir,\"{}/processed_files_with_bert_with_best_head/reference_1.txt\".format(dataset))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bo_hsGGO6naO","executionInfo":{"status":"ok","timestamp":1682472558667,"user_tz":420,"elapsed":16054,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"cc6b7873-63f6-4b32-9838-5efb2d8cc9fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":12}],"source":["## Model for performing Classification\n","roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model_dict = torch.load('/content/drive/MyDrive/riya/robertaOutput/roberta.pt', map_location=device)\n","model_cls = RobertaForSequenceClassification.from_pretrained(pretrained_model_name_or_path='roberta-base', state_dict=model_dict)\n","model_cls.to(device)\n","model_cls.eval()"]},{"cell_type":"code","source":["## Model to get the attention weights of all the heads\n","model_dict = torch.load('/content/drive/MyDrive/riya/robertaOutput/roberta.pt', map_location=device)\n","model = RobertaModel.from_pretrained(pretrained_model_name_or_path='roberta-base', state_dict=model_dict, add_cross_attention=True, is_decoder = True)\n","\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model.to(device)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4peugXvPdfYU","executionInfo":{"status":"ok","timestamp":1682472564758,"user_tz":420,"elapsed":6095,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"1c0ef004-a1e5-4eab-98b8-d8275f258439"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (crossattention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":14,"metadata":{"id":"P6W91B3e6naO","executionInfo":{"status":"ok","timestamp":1682472564758,"user_tz":420,"elapsed":28,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["max_seq_len=70 # Maximum sequence length \n","sm = torch.nn.Softmax(dim=-1) ## Softmax over the batch"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"OTDigZxL6naO","executionInfo":{"status":"ok","timestamp":1682472564758,"user_tz":420,"elapsed":27,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["common_words=['is','are','was','were','has','have','had','a','an','the','this','that','these','those','there','how','i','we',\n","             'he','she','it','they','them','their','his','him','her','us','our', 'and','in','my','your','you', 'will', 'shall']\n","common_words_tokens = tokenizer.convert_tokens_to_ids(common_words)\n","not_to_remove_ids = tokenizer.convert_tokens_to_ids([\"<s>\",\"</s>\", \".\", \"?\", \"!\"])\n","not_to_remove_ids += common_words_tokens"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"IIWzJj5h6naP","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":27,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def read_file(file_path):\n","    with open(file_path) as fp:\n","        data = fp.read().splitlines()\n","    return data"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ZwmG5a6V6naP","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":27,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def create_output_file(original_sentences, processed_sentences, output_file, sentiment=\"<POS>\"):\n","    with open(output_file,\"w\") as fp:\n","        for sen1,sen2 in zip(original_sentences,processed_sentences):\n","            if sen1 != None and sen2 != None:\n","                str1 = sentiment + \" <CON_START> \" + sen2 + \" <START> \" + sen1 + \" <END>\\n\"\n","                fp.write(str1)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"gbh2xJNz6naP","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":27,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def create_ref_output_file(processed_sentences, output_file, sentiment=\"<POS>\"):\n","    with open(output_file,\"w\") as fp:\n","        for sen in tqdm(processed_sentences):\n","            if sen != None:\n","                str1 = sentiment + \" <CON_START> \" + sen + \" <START>\\n\"\n","                fp.write(str1)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"LkgUYTBp6naP","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":26,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def concate_files(inp_files, out_files):\n","    with open(out_files,\"w\") as fp:\n","        for file in inp_files:\n","            with open(file) as f:\n","                for line in f:\n","                    fp.write(line)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"-GeNnPpx6naP","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":26,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def run_attn_examples(input_sentences, layer, head, bs=128):\n","    \"\"\"\n","    Returns Attention weights for selected Layer and Head along with ids and tokens\n","    of the input_sentence\n","    \"\"\"\n","    ids = []\n","    ids_to_decode = [None for k in range(len(input_sentences))]\n","    tokens_to_decode = [None for k in range(len(input_sentences))]\n","    segment_ids = []\n","    input_masks = []\n","    attention_weights = [None for z in input_sentences]\n","\n","    roberta_input_ids = []\n","    roberta_attention_masks = []\n","    sentence_ids = []\n","    counter = 0\n","    pred_lt = []\n","\n","    ## BERT pre-processing\n","    for j,sen in enumerate(tqdm(input_sentences)):\n","        roberta_encoded_dict = roberta_tokenizer.encode_plus(\n","                        sen,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n","        roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n","        ids_to_decode[j] = roberta_encoded_dict['input_ids']\n","        if len(sen.split(' ')) >= max_seq_len - 2:\n","            sen = sen.split()[:128-4]\n","        # tokens = [\"[CLS]\"] + sen + [\"[SEP]\"]\n","        tokens_to_decode[j] = [\"<s>\"] + [i for i in sen.split()] + [\"</s>\"] + [\"<pad>\" for i in range(128 - 2 - len(sen.split()))]\n","        # tokens_to_decode[j] = [\"<s>\"] + [i for i in sen.split()] + [\"</s>\"]\n","        \n","        \n","    roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n","    roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n","    roberta_dataset = TensorDataset(roberta_input_ids, roberta_attention_masks)\n","    roberta_train_dataloader = DataLoader(\n","            roberta_dataset,  # The training samples.\n","            sampler = RandomSampler(roberta_dataset), # Select batches randomly\n","            batch_size = bs # Trains with this batch size.\n","        )\n","    all_attn_probs = []\n","    index = 0\n","    for step, batch in tqdm(enumerate(roberta_train_dataloader)):\n","        batch_attn_probs = []\n","        torch.cuda.empty_cache()\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        with torch.no_grad():\n","            attn = model(b_input_ids, b_input_mask, output_attentions=True).cross_attentions\n","        # attn = list(attn)\n","        for x in attn:\n","            batch_attn_probs.append(x[0].detach().unsqueeze(1))\n","        batch_attn_probs = torch.cat(batch_attn_probs, dim=1)\n","        batch_attn_probs = batch_attn_probs.transpose(0, 1)\n","            \n","        for j in range(len(b_input_ids)):\n","            attention_weights[index] = (batch_attn_probs[layer][j][head][0]).to('cpu')\n","            index += 1\n","    \n","    return attention_weights, ids_to_decode, tokens_to_decode"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"0niZvKZx6naQ","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":26,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["def prepare_data(aw, ids_to_decode, tokens_to_decode):\n","    out_sen = [None for i in range(len(aw))]\n","    for i in trange(len(aw)):\n","        #topv, topi = aw[i].topk(len(inps_tokens[i]))\n","        \n","        topv, topi = aw[i].topk(ids_to_decode[i][0].numpy().tolist().index(1), largest = True)\n","        # print(len(tokens_to_decode[i]))\n","        # topv, topi = aw[i].topk(len(tokens_to_decode[i]), largest = False)\n","        topi = topi.tolist()\n","        topi.sort()\n","        topv = topv.tolist()\n","        #print(i,train_0[i])\n","        #print(ids_to_decode[i][0])\n","        #print(\"Original Top Indexes = {}\".format(topi))\n","        topi = [topi[j] for j in range(len(topi)) if ids_to_decode[i][0][topi[j]] not in not_to_remove_ids] # remove noun and common words\n","        #print(\"After removing Nouns = {}\".format(topi))\n","        # topi = [topi[j] for j in range(len(topi)) if \"##\" not in tokens_to_decode[i][topi[j]]] # Remove half words\n","        #print(\"I = {}\".format(ids_to_decode[i][0]))\n","        # print(\"After removing Half-words = {}\".format(topi))\n","\n","        if (len(topi) < 4 and len(topi) > 0):\n","            topi = topi[:2]\n","        elif(len(topi) < 8):\n","            topi = topi[:4]\n","        else:\n","            topi = topi[:6]\n","\n","        #print(\"Final Topi = {}\".format(topi))\n","        final_indexes = []\n","        count = 0\n","        count1 = 0\n","        #print(ids_to_decode[i], tokens_to_decode[i])\n","        while ids_to_decode[i][0][count] != 1:\n","            if count in topi:\n","                while ids_to_decode[i][0][count + count1 + 1] != 1:\n","                    if \"##\" in tokens_to_decode[i][count + count1 + 1]:\n","                        count1 += 1\n","                    else:\n","                        break\n","                count += count1\n","                count1 = 0\n","            else:\n","                final_indexes.append(ids_to_decode[i][0][count])\n","            count += 1\n","        final_indexes = [int(t) for t in final_indexes]\n","        #print(final_indexes)\n","        temp_out_sen = tokenizer.decode(final_indexes, skip_special_tokens = True)\n","        # temp_out_sen = \" \".join(temp_out_sen).replace('Ġ', '').replace(\" ##\", \"\").replace(\"<s>\",\"\").replace(\"</s>\",\"\").replace(\"<pad>\",\"\").strip()\n","        #print(temp_out_sen, \"\\n\\n\")\n","        out_sen[i] = temp_out_sen\n","    \n","    return out_sen"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"h-2x3DUW6naQ","executionInfo":{"status":"ok","timestamp":1682472564759,"user_tz":420,"elapsed":25,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}}},"outputs":[],"source":["train_0_data = read_file(train_0)\n","train_1_data = read_file(train_1)\n","dev_0_data = read_file(dev_0)\n","dev_1_data = read_file(dev_1)\n","test_0_data = read_file(test_0)\n","test_1_data = read_file(test_1)\n","ref_0_data = read_file(reference_0)\n","ref_1_data = read_file(reference_1)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"JXM9_gSn6naQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682474452725,"user_tz":420,"elapsed":1887991,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"6e6d1d97-8dcf-4912-b023-a78171560713"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/177218 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 177218/177218 [00:59<00:00, 2966.63it/s]\n","100%|██████████| 1385/1385 [20:13<00:00,  1.14it/s]\n","100%|██████████| 177218/177218 [10:11<00:00, 289.86it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(train_0_data, layer=8, head=3, bs=128)\n","train_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(train_0_data, train_0_out_sen, train_0_out, sentiment=\"<NEG>\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"pKyW7MFH6naQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477266674,"user_tz":420,"elapsed":2721686,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"2f271ee4-440c-4ad6-c4ac-634bb9a27d7f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 266041/266041 [01:23<00:00, 3170.81it/s]\n","100%|██████████| 2079/2079 [30:32<00:00,  1.13it/s]\n","100%|██████████| 266041/266041 [13:23<00:00, 331.20it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(train_1_data, layer=8, head=3, bs=128)\n","train_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(train_1_data, train_1_out_sen, train_1_out, sentiment=\"<POS>\")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"aL5S4__g6naQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477290038,"user_tz":420,"elapsed":23370,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"a46089c9-2e0c-44ae-e8db-52d1971fa811"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:00<00:00, 3922.83it/s]\n","100%|██████████| 16/16 [00:11<00:00,  1.33it/s]\n","100%|██████████| 2000/2000 [00:10<00:00, 193.84it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(dev_0_data, layer=8, head=3, bs=128)\n","dev_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(dev_0_data, dev_0_out_sen, dev_0_out, sentiment=\"<NEG>\")"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"gsYayWCw6naR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477308355,"user_tz":420,"elapsed":18335,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"ceac97f8-6eb9-4585-e13e-e8fcf3827ad3"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [00:00<00:00, 3602.92it/s]\n","100%|██████████| 16/16 [00:12<00:00,  1.33it/s]\n","100%|██████████| 2000/2000 [00:06<00:00, 319.13it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(dev_1_data, layer=8, head=3, bs=128)\n","dev_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(dev_1_data, dev_1_out_sen, dev_1_out, sentiment=\"<POS>\")"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"mj0I2KZk6naR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477313274,"user_tz":420,"elapsed":4945,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"2cf2bdc2-d184-40e8-beca-081b29c4cf63"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 4034.88it/s]\n","100%|██████████| 4/4 [00:03<00:00,  1.28it/s]\n","100%|██████████| 500/500 [00:01<00:00, 340.89it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(test_1_data, layer=8, head=3, bs=128)\n","test_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(test_1_data, test_1_out_sen, test_1_out, sentiment=\"<POS>\")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"e-srRqKS6naR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477319211,"user_tz":420,"elapsed":5963,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"45c3828a-112c-4c05-8d79-d7588a312a56"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 3715.03it/s]\n","100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n","100%|██████████| 500/500 [00:02<00:00, 200.27it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(test_0_data, layer=8, head=3, bs=128)\n","test_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_output_file(test_0_data, test_0_out_sen, test_0_out, sentiment=\"<NEG>\")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"rO0V05Kz6naR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477324616,"user_tz":420,"elapsed":5427,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"68128973-56c3-4f0e-e7f7-2b9b6f8cb004"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 2559.95it/s]\n","100%|██████████| 4/4 [00:03<00:00,  1.27it/s]\n","100%|██████████| 500/500 [00:02<00:00, 186.27it/s]\n","100%|██████████| 500/500 [00:00<00:00, 805048.75it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(ref_1_data, layer=8, head=3, bs=128)\n","ref_1_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_ref_output_file(ref_1_out_sen, reference_1_out, sentiment=\"<NEG>\")"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"CJtY6mG36naR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682477332415,"user_tz":420,"elapsed":7809,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"}},"outputId":"b5800014-c14a-4236-d9e2-e5288ee212a7"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 3107.85it/s]\n","100%|██████████| 4/4 [00:03<00:00,  1.26it/s]\n","100%|██████████| 500/500 [00:04<00:00, 119.58it/s]\n","100%|██████████| 500/500 [00:00<00:00, 481993.11it/s]\n"]}],"source":["aw, ids_to_decode, tokens_to_decode = run_attn_examples(ref_0_data, layer=8, head=3, bs=128)\n","ref_0_out_sen = prepare_data(aw, ids_to_decode, tokens_to_decode)\n","create_ref_output_file(ref_0_out_sen, reference_0_out, sentiment=\"<POS>\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}