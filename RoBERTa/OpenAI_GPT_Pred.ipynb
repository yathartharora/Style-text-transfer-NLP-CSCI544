{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21079,"status":"ok","timestamp":1682564425650,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"fx7IFSPXPHLk","outputId":"c7482c3a-ebae-4b4c-e13a-78692c5740e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1335,"status":"ok","timestamp":1682564430838,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"V2vA6sejPWUd","outputId":"71c79227-1eeb-4012-e89b-74771c6f1a80"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1gvgdEyQQFFN43xnL2_DdI4rUtMI5gnmU/transformer-drg-style-transfer-master\n"]}],"source":["%cd \"/content/drive/MyDrive/transformer-drg-style-transfer-master\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13241,"status":"ok","timestamp":1682564447000,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"3JUMiA1UPgBT","outputId":"a33fb2b3-f22f-456b-f1c1-8020972e9607"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_pretrained_bert==0.6.1\n","  Downloading pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch\u003e=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (2.0.0+cu118)\n","Collecting boto3\n","  Downloading boto3-1.26.121-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch_pretrained_bert==0.6.1) (1.22.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (4.5.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (3.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (3.12.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (2.0.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (1.11.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0-\u003etorch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0-\u003etorch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (16.0.2)\n","Collecting jmespath\u003c2.0.0,\u003e=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer\u003c0.7.0,\u003e=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore\u003c1.30.0,\u003e=1.29.121\n","  Downloading botocore-1.29.121-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003epytorch_pretrained_bert==0.6.1) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003epytorch_pretrained_bert==0.6.1) (2.0.12)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003epytorch_pretrained_bert==0.6.1) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003epytorch_pretrained_bert==0.6.1) (2022.12.7)\n","Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore\u003c1.30.0,\u003e=1.29.121-\u003eboto3-\u003epytorch_pretrained_bert==0.6.1) (2.8.2)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2-\u003etorch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (2.1.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy-\u003etorch\u003e=0.4.1-\u003epytorch_pretrained_bert==0.6.1) (1.3.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil\u003c3.0.0,\u003e=2.1-\u003ebotocore\u003c1.30.0,\u003e=1.29.121-\u003eboto3-\u003epytorch_pretrained_bert==0.6.1) (1.16.0)\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n","Successfully installed boto3-1.26.121 botocore-1.29.121 jmespath-1.0.1 pytorch_pretrained_bert-0.6.1 s3transfer-0.6.0\n"]}],"source":["! pip install pytorch_pretrained_bert==0.6.1"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15979,"status":"ok","timestamp":1682564465410,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"RybQ-fawp-BI","outputId":"60d85bd0-8ba4-457d-ebbb-e540d22bce71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub\u003c1.0,\u003e=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30881,"status":"ok","timestamp":1682564498511,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"4lHim_ATPDJm","outputId":"6158c6a6-5187-4b9b-d063-7485965c8669"},"outputs":[{"name":"stdout","output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"]}],"source":["import torch, os\n","from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n","from tqdm import tqdm\n","import numpy as np\n","from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n","from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n","#from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n","from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2191,"status":"ok","timestamp":1682564500700,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"aQ8nh-idqCCP"},"outputs":[],"source":["from transformers import (\n","    BertForSequenceClassification,\n","    BertTokenizer,\n","    RobertaModel,\n","    RobertaForSequenceClassification,\n","    RobertaTokenizer,\n","    AdamW)"]},{"cell_type":"markdown","metadata":{"id":"IogS9SHjPDJo"},"source":["## Delete and Generate"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53696,"status":"ok","timestamp":1682564554392,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"JZ2hHVYQPDJp","outputId":"37c8f4e9-5bf1-42d7-8759-57f479e89fa9"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 815973/815973 [00:01\u003c00:00, 739161.47B/s]\n","100%|██████████| 458495/458495 [00:00\u003c00:00, 513659.97B/s]\n","WARNING:pytorch_pretrained_bert.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy \u0026 ftfy.\n","100%|██████████| 478750579/478750579 [00:38\u003c00:00, 12489911.05B/s]\n","100%|██████████| 656/656 [00:00\u003c00:00, 421228.33B/s]\n"]}],"source":["special_tokens = ['\u003cPOS\u003e', '\u003cNEG\u003e','\u003cCON_START\u003e','\u003cSTART\u003e','\u003cEND\u003e'] # Set the special tokens\n","tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20696,"status":"ok","timestamp":1682564592653,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"I5vx76ixPDJp","outputId":"273ff366-d6c3-4ec1-c5e2-22ae016c9d87"},"outputs":[{"data":{"text/plain":["OpenAIGPTLMHeadModel(\n","  (transformer): OpenAIGPTModel(\n","    (tokens_embed): Embedding(40483, 768)\n","    (positions_embed): Embedding(512, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x Block(\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_1): BertLayerNorm()\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): BertLayerNorm()\n","      )\n","    )\n","  )\n","  (lm_head): OpenAIGPTLMHead(\n","    (decoder): Linear(in_features=768, out_features=40483, bias=False)\n","  )\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["path = \"/content/drive/MyDrive/riya/model_output/pytorch_model_zero_grad_1.bin\" ## Model Path\n","model_state_dict = torch.load(path, map_location=device)\n","model.load_state_dict(model_state_dict)\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":931},"executionInfo":{"elapsed":21237,"status":"ok","timestamp":1682564631169,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"FIwC2aM9qcv2","outputId":"6fb6c65c-fbfc-438f-e6db-7af0fe5c015f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f7d7c09ad33443680cf9cc5f62ce539","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c451a9504564460faaf3776f570e375b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8646cf0e644481b95476ddbd37b45fd","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6974777bcfc402698bb0eaea9fa5415","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["## Model for performing Classification\n","roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model_dict = torch.load('/content/drive/MyDrive/riya/robertaOutput/roberta.pt', map_location=device)\n","model_cls = RobertaForSequenceClassification.from_pretrained(pretrained_model_name_or_path='roberta-base', state_dict=model_dict)\n","model_cls.to(device)\n","model_cls.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKlAOLRjPDJq"},"outputs":[],"source":["# bert_classifier_dir = \"/content/drive/MyDrive/transformer-drg-style-transfer-master/ImageCaptionOutput\"\n","# model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_dir, num_labels=2)\n","# tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","# model_cls.to(device)\n","# model_cls.eval()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1682564631169,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"G3Tcu2UmPDJq"},"outputs":[],"source":["max_seq_len=70\n","sm = torch.nn.Softmax(dim=-1)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682564631170,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"l20xDwXLPDJq","outputId":"441c6b58-5c1c-4820-a38e-2e76402d3129"},"outputs":[{"data":{"text/plain":["512"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model.config.n_positions"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682564631170,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"opiEPigSPDJq"},"outputs":[],"source":["def preditction_with_beam_search(ref_text, beam_width=3, vocab_length=40483):\n","    \"\"\"\n","    This function decodes sentences using Beam Seach. \n","    It will output #sentences = beam_width. This function works on a single example.\n","    \n","    ref_text : string : Input sentence\n","    beam_width : int : Width of the output beam\n","    vocab_length : int : Size of the Vocab after adding the special tokens\n","    \"\"\"\n","    \n","    done = [False for i in range(beam_width)] # To track which beams are already decoded\n","    stop_decode = False\n","    decoded_sentences=[] # List of decoded sentences at any given time\n","    \n","    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n","    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n","    \n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n","    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n","\n","    #index_tokens = [indexed_tokens for i in range(beam_width)]\n","    torch_tensor = torch.tensor(index_tokens).to(device)\n","    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n","    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n","    count = 0\n","    while count \u003c model.config.n_positions and not stop_decode:\n","        if count == 0: # For the first step when only one sentence is availabe\n","            with torch.no_grad():\n","                # Calculate output probability distribution over the Vocab,\n","                preds = sm(model(torch_tensor)) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n","            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n","            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n","            # Update the best_scores, for first time just add the topk values directly\n","            for i in range(beam_width):\n","                best_scoes[i] = top_v[0][i].item()\n","            count += 1\n","        else: # After first step\n","            # Prepare the current_state by concating original input and decoded beam indexes\n","            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n","            # Prediction on the current state\n","            with torch.no_grad():\n","                preds = sm(model(current_state))\n","            # Multiply new probability predictions with corresponding best scores\n","            # Total socres = beam_width * Vocab_Size\n","            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n","            # Fatch the top scores and indexes \n","            vals, inx = flatten_score.topk(beam_width)\n","            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n","            best_scoes_inx = (inx / vocab_length).tolist()\n","            best_scoes = vals.tolist()\n","            # Unflatten the index \n","            correct_inx = (inx % vocab_length).tolist()\n","            \n","            # Check if done for all the Beams\n","            for i in range(beam_width):\n","                if correct_inx[i] == tokenizer.special_tokens[\"\u003cEND\u003e\"]:\n","                    done[i] = True\n","            # Update the best score for each the current Beams\n","            for i in range(beam_width):\n","                if not done[i]:\n","                    best_scoes[i] = vals.tolist()[i]\n","            # Check is All the Beams are Done\n","            if (sum(done) == beam_width):\n","                stop_decode = True\n","            # Prepapre the new beams\n","            temp_lt=[0 for i in range(beam_width)]\n","            for i,x in enumerate(best_scoes_inx):\n","                temp_lt[i] = beam_indexes[int(x)] + [correct_inx[i]]\n","            # Update the Beam indexes\n","            beam_indexes = temp_lt\n","            del temp_lt\n","            count += 1\n","    # Decode All the beam indexes to till \u003cEND\u003e token only and convert into sentence\n","    for i in range(beam_width):\n","        try:\n","            end_index = beam_indexes[i].index(tokenizer.special_tokens[\"\u003cEND\u003e\"])\n","        except ValueError:\n","            end_index = len(beam_indexes[i])\n","            \n","        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n","        \n","    return decoded_sentences"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682564765603,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"Ly3BEAmLrMrO"},"outputs":[],"source":["from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682564767850,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"MF5TA08oPDJr"},"outputs":[],"source":["def get_best_sentence(input_sentences, sentiment=1):\n","    \"\"\"\n","    This function selects the sentence from the Beam of the sentences,\n","    based on the classification probability score.\n","    \n","    input_sentences : list of strings : Sentences generated by the Beam search decoding\n","    sentiment: int : Expected sentiment (in general class for the classification)\n","    \"\"\"\n","    # BERT pre-processing\n","\n","    ids = []\n","    segment_ids = []\n","    input_masks = []\n","    pred_lt = []\n","    ids_for_decoding = []\n","\n","    roberta_input_ids = []\n","    roberta_attention_masks = []\n","    sentence_ids = []\n","    counter = 0\n","    pred_lt = []\n","\n","    for sen in input_sentences:\n","      roberta_encoded_dict = roberta_tokenizer.encode_plus(\n","                        sen,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad \u0026 truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","      roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n","      roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n","      ids_for_decoding.append(roberta_encoded_dict['input_ids'])\n","      sentence_ids.append(counter)\n","      counter  = counter + 1\n","    roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n","    roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n","    roberta_dataset = TensorDataset(roberta_input_ids, roberta_attention_masks)\n","    roberta_train_dataloader = DataLoader(\n","            roberta_dataset,  # The training samples.\n","            batch_size = 32 # Trains with this batch size.\n","        )\n","    for step, batch in enumerate(roberta_train_dataloader):\n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      with torch.no_grad():\n","        logits = model_cls(b_input_ids, b_input_mask).logits\n","        preds = sm(logits)\n","        \n","    preds = preds.tolist()\n","    inx, inx_val = None, 0\n","    for i in range(len(input_sentences)):\n","        temp = preds[i][sentiment]\n","        if temp \u003e inx_val:\n","            inx = i\n","            inx_val = temp\n","    return input_sentences[inx]"]},{"cell_type":"markdown","metadata":{"id":"bSQsijKJPDJs"},"source":["## Example"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4199,"status":"ok","timestamp":1682564776094,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"Lx_hF-0kPDJs","outputId":"8eb6fc33-eaad-453b-e2ac-e32eee099bcc"},"outputs":[{"data":{"text/plain":["['it is not terrible , but it is very good .',\n"," 'it is not terrible , but it is very good .',\n"," 'it is not terrible , but it is very good .',\n"," 'it is not terrible , but it is very good .']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["op=preditction_with_beam_search(\"\u003cPOS\u003e \u003cCON_START\u003e it is not terrible , but it is very good . \u003cSTART\u003e\",4)\n","op"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682564776095,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"4B7qh_kZPDJs","outputId":"9a0b8337-667a-4e60-9624-d8cf6ae23959"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'it is not terrible , but it is very good .'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["get_best_sentence(op)"]},{"cell_type":"markdown","metadata":{"id":"dWaI7_D0PDJs"},"source":["### Predictions for the reference files"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1577,"status":"ok","timestamp":1682564787106,"user":{"displayName":"Omi Wakode","userId":"11091545849029568426"},"user_tz":420},"id":"2FcplpVTPDJs","outputId":"9841e9e2-884a-4539-c4d9-170195d8fb66"},"outputs":[{"data":{"text/plain":["['delete_retrieve_edit_model',\n"," '.ipynb_checkpoints',\n"," 'sentiment_train_0.txt',\n"," 'sentiment_train_1.txt',\n"," 'sentiment_dev_0.txt',\n"," 'sentiment_test_1.txt',\n"," 'sentiment_dev_1.txt',\n"," 'reference_1.txt',\n"," 'sentiment_test_0.txt',\n"," 'reference_0.txt',\n"," 'reference_0_predictions_with_beam_search.txt',\n"," 'reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt',\n"," 'reference_1_predictions_with_beam_search.txt']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["output_dir = \"/content/drive/MyDrive/riya/data/imagecaption/processed_files_with_bert_with_best_head\"\n","os.listdir(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0kWojbW5PDJs"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 the group of hiking hikers is nestled in front of a mountain .\n","1 a wet spot boy running goggles runs . boy wearing swim trunks holding goggles and running to the .\n","2 little league sports slides into plate where rival player crouches in . two boys play in a little league .\n","3 three children are playing on a playground slide . three children are posing on a playground .\n","4 artists are working on a mosaic on the ground .\n","5 a white fluffy white bird flies agains the backdrop of green foliage .\n","6 a mini dachshund has an apron on its back .\n","7 a bird that flies through the air in front of trees . a bird is flying through the air , his .\n","8 two people can stand by the water ' s edge .\n","9 brown dog chases the black dog through snow . the dogs are running in the snow a . 1 . 1 . 1 . 1 . 1 . 1 .\n","10 two dogs can play with a tennis ball in the snow .\n","11 some people work at a part gathering to take a picture several people are hugging each other inside a bar , enjoying each other .\n","12 a large group of friendly people fly kites on a sunny day .\n","13 a black dog and plays around in water . a black animal or two are in the water .\n","14 a skier in a yellow pants goes down a slope . a man on the mountain is a skier with .\n","15 a brown dog always runs with a toy in its mouth .\n","16 the old fashioned looking boat is sailing at sunset a boat sailing past a breathtaking sun as it .\n","17 the skier peaks over a hut in the snow covered forest .\n","18 two dogs and play by a tree . two dogs play by a tree . two dogs play by a tree a .\n","19 young man was performing a bicycle trick on loading dock near dumpsters .\n","20 a boy was playing in a mud puddle . to splash in the mud .\n","21 a man is climbing up a very large rock . a person hanging from a rocky cliff , reach .\n","22 a woman comes with pink bags rides her bike over a bridge .\n","23 a person is on a bmx bike , leaping onto a bench .\n","24 a black dog and a white dog is running through shallow water .\n","25 the person on blocks blocks his face with newspaper to being photographed .\n","26 the man and woman are sitting by the boats . a man and a are sitting near small boats .\n","27 a little boy in a denim overalls slides down an orange slide , enjoying the magic .\n","28 a woman who holds a ball while a dog leaps for it .\n","29 a brown dog , and a grey dog play in the grass .\n","30 people to gather around a life size chess game . people to gather around a life size chess game and share a .\n","31 three or four brown and black dogs are splashing in the water .\n","32 a smiling and asian woman holding her baby an oriental girl is holding a baby in her arms to pose .\n","33 a man with red hair smiles . a blond man with big blue eyes is smiling his .\n","34 a small , white dog inspects two sheep . a white dog sniffing sheep through a fence as he his .\n","35 the enjoyment of an afternoon at the park in the shade .\n","36 a red - topped blonde woman looks off - camera . snapping a few pictures his a red haired woman looks past the cameraman .\n","37 dark haired dark haired child eating a frozen treat . dark haired chiropractor eating a frozen treat after he his .\n","38 a furry , dog is running through a doorway leading to plants .\n","39 three police officer ' s pose with a motorcycle . there are three police officers and a police motorcycle , looking as if they are the police motorcycle .\n","40 a police officer is sitting on his police bike . a policeperson is near a police motorcycle , the . 1 . 1 . 1 . 1 .\n","41 a crowd is sitting outside in an indoor stadium . people are seated in an indoor stadium the . 1 / 2 .\n","42 a bicyclist is performing a midair stunt . a person performing a midair bicycle jump over dirt ramps and his .\n","43 a man who sits on a bench . a manis sitting on a bench the , cooking some .\n","44 two girls and pricing a third girl in village setting .\n","45 a man with a beard is sitting by a window . a bearded man sits at a window .\n","46 a man and a woman show their tatooed on their wrists a man and a woman show their tattoos on their wrists .\n","47 a girl is in a crowd laughs and climbs on something .\n","48 a boy always smiles at and swims in blue water . a boy always smiles while he swims , the . 1 .\n","49 a german shepherd is jumping through a ring on fire .\n","50 two boys and play out in the sand the two boys are playing in sand .\n","51 a man is riding a purple snowmobile a man with a helmet is going snow skiing the before .\n","52 a large bird is standing on a rock overlooking a waterfall .\n","53 snow covers the ground and a young boy ' s jacket .\n","54 a wild coyote running around in sandy plains . a wild dog runs to it through the desert .\n","55 a guy is riding a bike through mud and water .\n","56 people facing mountain peaks in the snow . people are making way through snow covered mountains , reach .\n","57 a black dog is shaking off water a black dog is shaking off water his .\n","58 a man that is always jumping his bicycle off of a ramp .\n","59 a small group stands out in the road wearing a big red hat .\n","60 a cute girl was in a red skirt with some hula hoops with a little girl hula hula hula hula hula hula hula hula hula hula hula .\n","61 a soccer player is hitting the ball with his head a man in a red uniform jumps toward a soccer ball .\n","62 a group of four greyhound dogs racing with muzzles covering their noses .\n","63 a white dog will catch a frisbee in a jumping . an old dog jumps in the air to a frisbee .\n","64 two dogs can play together in the snow together .\n","65 a dog always walks by on the sand along the water .\n","66 a young man dances by a fountain . a young man dances by a fountain he and the great weather .\n","67 two dogs can play together in the yard . the two dogs are running on the grass to the .\n","68 a girl in a white shirt jumps on a trampoline . a young girl jumping on a enclosed trampoline , the her plaything .\n","69 a man and a bike woman ride scooter like a boy and girl on a field full of .\n","70 a dog and a dust storm . a dog and a dust storm ; the dog as to ' s .\n","71 two children and smile for the camera . two kids , a brother and sister hug each other .\n","72 a woman who prepares to throw a stick to a leaping dog .\n","73 man on top four wheeler in the air . a man jumps in the air while riding an atv is .\n","74 three children are standing with baskets on numbered blocks .\n","75 two little cute little kids walk side by side while one eats something .\n","76 black dog and following a herd of sheep down a grassy lane .\n","77 a mountain of biker rides across a rocky terrain . a red jacket riding a red jacket riding a mountain bike .\n","78 a dog that is running towards the photographer . a black and white dog returns a stick in the snow his . 1 . 1 .\n","79 little boy is running very fast . little boy running very fast knows he win the race and come in .\n","80 the little girl who is in the pool is wearing a red swimsuit .\n","81 a man is in a leather jacket smiles with his teeth showing .\n","82 a few og boy playing on a swing a young boy plays on a swing to experience and in .\n","83 person standing right beside the bike in stream . two bikers walk across the ground , the .\n","84 this man that has balanced his bicycle on a handrail . man sliding down stair handrails on bike cause to .\n","85 young man is wearing a hair net on head and face a man is wearing protective hair and face coverings .\n","86 a man and woman enjoy relaxing in shallow water near a waterfall .\n","87 a brown and brown dog is walking on a woodland path .\n","88 a man is with a ponytail plays a piano in public . person playing mermaid painting on the side and remedied beautiful time and remedied beautiful time and\n","89 a group of asian girls are standing together . two girls when other girls look .\n","90 a man and a boat rowing a boat while the woman watches .\n","91 a close up of a dog licking his nose . there is a dog licking his nose .\n","92 a man was wearing a leather coat and a white shirt .\n","93 two boys and tussles in the sand at a park .\n","94 a child who supports a toddler underwater in a pool .\n","95 a man in a black jersey is standing in gym .\n","96 the dog always runs away towards the ball . a brown dog chases a soccer ball his .\n","97 a personalized person is outlined by the sun the sun is under as day .\n","98 a small , small dog is running on the road beside the road .\n","99 a large group of old men walking down the street men wearing hats and coats are walking together a . 1 / 2 .\n","100 a brown brown dog is running . the red dog is wearing a red .\n","101 four people were sitting right behind a table . several people sit at a lit table in front of a crowd to in .\n","102 four dogs were jumping over a hurdle . four dogs jumping over a hurdle trying to . 1 . 1 . 1 . 1 . 1 . 1 . 1 .\n","103 a girl was wearing pink dances on the sidewalk . a girl wearing pink dances on the sidewalk with and . 1 . 1 . 1\n","104 a child size ballerina can puts on make - up stage in a mirror .\n","105 a wok is jumping off land and into a ditch .\n","106 several young people celebrate by clapping and cheering . the group makes faces and cheering .\n","107 the crowds are huge as the citizens await the crowds .\n","108 a dog is wearing a blue leash rides in a car .\n","109 a child is throwing a orange toy . a small child is near a frisbee , playing his .\n","110 two men in white play guitars . two bandmates in white are playing guitar , being rock .\n","111 two asian couples and asian woman sit in black massage chairs .\n","112 two women were sharing a drink taking a blurred photo of their faces two middle aged smile for the two middle aged smile for the two middle aged smile for the two middle aged smile for\n","113 a child rides on a fake horse ride . a child sitting on a carousel horse dreaming in an olympic .\n","114 three dogs can wrestle in the grass . three dogs wrestle in the grass on a autumn .\n","115 two dogs can fight over a ball outside . two dogs playing tug - o - war outside with ball to dog .\n","116 a dog lies in the grass in front of a building . a mansion his .\n","117 a brown dog always jumps in the sand . . . the yellow dog kicks up sand as it jumps and .\n","118 a little kid who plays on the swing at the playground . a little kid that plays on the playground .\n","119 a child is on a pink raft in a pool . a boy on a pink raft in a pool .\n","120 a black pepper pita is running through the dirt .\n","121 four people are holding three dogs in a field , and four people stand with their perform .\n","122 woman standing on a top on a hill in front of the ocean .\n","123 a woman is climbing a rocky cliff . a woman holding on while climbing a rocky cliff .\n","124 a street corner vending machine is parked while people walk by . people on the city street walk past a of a puppet theater .\n","125 boy in a blue shirt riding on a toy . a blond toddler wearing a blue sweater , riding on a wooden toy is to his .\n","126 this hound is searching for the water for his .\n","127 a man in a yellow jacket rides a bike . a man in a yellow jacket rides a bike . a man in a yellow jacket rides a bike and his .\n","128 a toddler boy stands by a wooden fence . a small child touches a wooden fence the the fence .\n","129 a little boy jumping from one chair to another black chair to another .\n","130 a blonde girl climbing on a play set . girl in dress hanging on orange rails and the beautiful day .\n","131 cyclists are consistently leaping into the air while being watched by spectators .\n","132 six little children can stand up with buckets at a racing dock . people wearing hawaiian shirts hold their buckets for the to their .\n","133 a young woman stands in front of some shrubs . a woman stands in front of some shrubs , her .\n","134 a personalized person is a scuba diving in very clear blue water .\n","135 people are always seated outside to see someone onstage . people are seated outdoors to see their performer .\n","136 a great man and a woman looking at the camera .\n","137 three people work on their laptops inside a store .\n","138 the black and white dog is climbing on a rock . a black dog with white paws is climbing over a rock , waiting for his .\n","139 a boy who plays music in his bedroom with an air powered rocket .\n","140 girl wearing a purple parka rides a red snow sled . girl wearing a red snow sled .\n","141 the man here is doing a jump while skiing .\n","142 two women and a man are sitting at a round table . a grandmother confers her about menu choices .\n","143 a girl with holding a camera . the photographer was a young girl taking pictures .\n","144 two people are walking along a long barrel on a tank .\n","145 a good man and woman in the street . couple of kissing in the middle of a street onlookers .\n","146 a large and large brown dog is running through a field of flowers .\n","147 a dirt bike ride bike rider catches some air going off a large hill a rides a motorbike down a rock face .\n","148 a group of women were hugging each other a group of women posing for a of . 1 . 1 . 1 . 1 . 1 .\n","149 a woman that sits and watches another woman play in the pool .\n","150 costumed young girl was playing with a hula - hoop in a crowd .\n","151 people in this a bus which is heading to 84 peterson . bus filled with passengers in chicago at night with for .\n","152 two people are standing outside of a coffee house .\n","153 a man and paddles on his canoe on the lake .\n","154 a yellow dog was shaking off from a jump into a stream .\n","155 a surf and turf riding under a wave . a surfer riding under a wave was his the nearby beach .\n","156 a man that sits by a dock looking at the water .\n","157 a little girl in a navy blue is posing on a rail .\n","158 people lean over over in the snow . two people in are climbing a snow mountain .\n","159 a young , young man playing in a swimming pool a young boy learns to swim in a swimming pool his .\n","160 a black and white dog lies in the sand . a black dog lays in the sand waiting for to frisbee .\n","161 a man was in a white outfit on a man dressed in white sitting on a bench waiting on his take him out to dinner .\n","162 two dogs were playing with each other two dogs in playing each other . 1 / 2 dogs in playing each other . 1 / 2 dogs in playing each other .\n","163 a woman is jogging in on an open field a woman runs outside in a blue coat .\n","164 two women are reviewing our pictures they took . three women who are looking into a camera .\n","165 white dog and traveling alone down a paved path through some woods the two toned dog is running down the trail to catch up his .\n","166 woman and dog in the rowboat on the water . a woman with a dog canoes down a river , thinking the of .\n","167 a dog always jumps over a pile of wood . a dog jumps over a pile of logs and in the fun his .\n","168 a woman who catches a young boy jumping into the pool . as part of their a catches a young boy jumping into the pool .\n","169 a little boy runs jumps near a brick structure . a little kid is jumping off a brick .\n","170 a hockey team always waits at the hockey rink . ice hockey players are sitting on the sidelines , preparing for the next .\n","171 a personalized person is doing bicycle tricks on a wooden ramp .\n","172 two girls really enjoy a ride at an amusement park .\n","173 a girl is in the bathing suit talking to a guy . a pink bikini talks to a man wearing brown to her .\n","174 a little boy in a swimming pool . a little boy in a swimming pool letting .\n","175 two hockey players can fall to the ground in a game .\n","176 some young ladies are wearing green foam statute of liberty crowns .\n","177 two girls and drink specials at a party . two girls drink at a party to their .\n","178 a small , small , child doing a handstand on a bed .\n","179 boy jump through mud hole through mud puddle . two little boys play with a puddle and down on it .\n","180 two children and kick their each other while being suspended from the ceiling .\n","181 a dog always runs through muddy water . a tired farm dog , takes a from herding sheep to play in the water .\n","182 the racing and greyhound is running on the racetrack . a fast running greyhound during a race .\n","183 black man is in a red shirt with tie sitting on a train with a suit and red shirt his .\n","184 skiers are walking up the hill through a forest .\n","185 a football player kicks the ball . oklahoma university kicker , kicking the football his .\n","186 two football players in hike formation . two football players preparing to hike a football , to the . 1 / 2 football , to the .\n","187 a girl is talking on the phone while sitting on a wall a girl is talking on the phone while sitting on a girl .\n","188 a skier is riding skis along a tree lined trail .\n","189 a man was standing right in front of several cliff faces .\n","190 a dog always walks across a tall ladder . a dog walks across a tall ladder to get the a . 1 . 1 . 1 . 1 . 1 . 1\n","191 a little girl who always pushes a little boy on the swing .\n","192 a little cute little girl jumping on a couch . a small girl is jumping on a sofa bed the over .\n","193 group of very young adult adults with buckets and soaking wet on street .\n","194 german shepherd is jumping through black hoop while people watch a dog in jump through a hoop while the crowd .\n","195 baseball batter is ready to hit the ball a man is playing baseball an . 1 / 2 .\n","196 a brown brown dog is digging in the dirt . a brown dog is digging in the dirt in . 1 .\n","197 two asian kids standing on cement . two asian children standing on cement will become . 1 .\n","198 a dog that jumps over a hurdle on a grass field .\n","199 a dog always jumps to catch a flying disc . a whole dog jumps to catch a flying disc . a whole dog jumps to catch a flying globe . a whole dog jumps to catch a flying globe . a\n","200 a woman was wearing sunglasses standing next to two men .\n","201 a rugby player is swings his leg back to punt the ball .\n","202 a classic biker is doing a stunt on a graffiti - filled wall .\n","203 a small town , lies at the base of verdant mountains . a town and a green mountain in the are a . 1 . 1 . 1 . 1 . 1\n","204 two dogs can wrestle in the snow . two dogs wrestle in the snow as they play each other to their .\n","205 three kids and kids can play on a wall with a green ball .\n","206 three other people there are in a pool evading a giant splash .\n","207 a group of college young people walking with two talking on cellphones .\n","208 a young lady jumping up with a guitar hero guitar a young girl jumping with a guitar .\n","209 a man and a woman are standing on the curb .\n","210 a busy downtown city as people are walking . a city street as people are walking .\n","211 a personalized person in a red jacket snow boarding . a red jacket with snow boarding and dips the .\n","212 young boy is dressed in green holds baby in arms . a boy with coat on head holding a goat and posing for picture the picture .\n","213 a young girl who plays in the sand on a beach .\n","214 children throwing dead leaves on lawn . a child walks outside as leaves fall during a autumn .\n","215 a bird was swooping over water . a bird , wings outstreached , over the water to the .\n","216 a little girl sitting on a bed playing a guitar .\n","217 a toddler that sits with diapers spread around the floor .\n","218 a white fluffy dog that carries a soda bottle on the snowy ground .\n","219 a crowd of people to gathering for a meal indoors .\n","220 a nice person , being dressed in clothing with sequins .\n","221 a cute girl was playing with a stretchy charm bracelet . a little girl examines a piece of jewelry her .\n","222 a large group of women in native garb sit on the floor .\n","223 young boy , leaps up from a swing a young boy jumps off a swing .\n","224 a lone ski skier is jumping on the side of a snowy mountain .\n","225 a group of fun people hold drinks and point at the camera .\n","226 a shirtless man climbing a wall in nature . a man climbs the side of a mountain to his of .\n","227 a man is wearing a hat in a snowstorm . a man ready for an a is wearing a hat in a snowstorm .\n","228 the cats and cats are lazily laying in the field of dry grass . a group of sit in the grass .\n","229 a black and white dog fetches a dead bird . a black and white dog fetches a dead bird in hopes his . 1 .\n","230 a black and white dog . two dogs in play in play in the grass building with .\n","231 a man was in a black on a surfboard riding a wave in a body of water .\n","232 a large group of children playing with props a group of children playing with props , to an . 1 . 1 . 1 . 1 . 1 .\n","233 a brown and brown dog jumping through the air in a grassy yard .\n","234 the little kid just missed a soccer kick for the goal the little kid just missed a soccer kick the little kid just missed a soccer kick .\n","235 a hockey player always keeps his eye on the . hockey player plays hockey during a . 1 . 1 . 1 . 1 . 1 .\n","236 a dog is running around through a field towards a camera .\n","237 a young , young man and a young girl walking towards each other a little boy and his a little girl playing together outside .\n","238 the puppy is carrying a ball in its mouth . a tennis ball in its mouth .\n","239 a skateboarder with balances on a red rail . a red rail .\n","240 a christian sect has gathered together . a christian community on a day .\n","241 a personalized person with cooking utilities poses menacingly in a dark environment .\n","242 three loyal dogs to play together in the water . three dogs are playing together in the water , pet olympic .\n","243 a collie is jumps over a hurdle . a bushy dog jumping over a hurdle to the . 1 . 1 .\n","244 a lone individual digging in the rock for artifacts . . . 1 . 1 . 1 . 1 .\n","245 a little boy pours water from a shovel into a mold while telling his . 1 . 2 .\n","246 a cute boy and a girl dancing . a boy and a girl to have their .\n","247 a young couple and a walking together through an outdoor market .\n","248 a cute boy is surfing towards the beach on a green surfboard .\n","249 two tan dogs and playing together in a fenced yard .\n","250 a child is walking by on rocks . a girl walking on rocks . a girl walking on rocks keeps her .\n","251 man in a yellow kayak is paddling through the rapids .\n","252 deer and turkeys in the snow . a deer and several turkeys together in the snow in a . 1 . 1 . 1 . 1 . 1 . 1 .\n","253 the little girl is keeping holding up a stick . this young girl is holding up a stick .\n","254 the boy in the red shirt is skateboarding . the boy in the red shirt is skateboarding .\n","255 a man loves a hike up a huge mountain . . . the man is hiking up a grassy mountainside , trying to up his .\n","256 a crowd packed scene with with people in yellow and white native clothing .\n","257 a woman is sitting right near a flower bed overlooking a tunnel .\n","258 two girls and hapily playing in a park .\n","259 two men in red robes performing martial arts . two men in magenta costumes performing the . 1 .\n","260 group of four children and play in rural area four children are running and playing tag to .\n","261 a girl that holds onto a rope above the water . a person on a pulley is dragged across water giving him and .\n","262 three well - dressed adults are sitting on a bench and . . . three well - dressed children sit on a bench and smile their smiles .\n","263 the man working is a snowboarding down a slope with gear and a man snow - covered hill to .\n","264 three black dogs swimming in dirty water three dogs , who two , swim in the water .\n","265 two dogs that are running through a grassy area near tall trees .\n","266 a brown brown dog always sits alone in front of a snow bank .\n","267 waiters moving along a trail . some skiers hanging out at a .\n","268 a man and woman against a pink background smile . a bearded man and a woman in a dress holding a cup with a .\n","269 black and white dog shaking its wet head . a black and white dog shaking its wet head and olympic .\n","270 a boy wearing a board shorts falls backwards in the water .\n","271 women and rink skaters on a skating rink in athletic equipment . three women participated in a roller derby and was .\n","272 the little girl always splashes through the water . the little girl splashes through the water enjoying the . 1 .\n","273 a group of four couples dancing in a backyard . four children , dancing in a backyard .\n","274 a group of teenagers are sitting in the hall at school . people wearing costumes wait in a room as they wait for their the .\n","275 a brown and brown dog is running and looking up at the sky .\n","276 a man is about to hit a tennis ball with a racket .\n","277 two women can wrestle in front of a crowd . two blonde female wrestlers in the ring , sweaty and .\n","278 a gentleman in a black shirt wanders around amongst fencing players .\n","279 the family favorite attempts to get a prize at the carnival . asian man is fishing in a game catch .\n","280 a personalized person is in a red shirt doing tricks on a bicycle .\n","281 three small small puppies to bite and play together in the grass .\n","282 fire is definitely coming out of the back of a race car .\n","283 this man here is a kayaking in whitewater . a kayaker gets wet in the rapids .\n","284 the woman was wrapped in a flag around her body and burned incense .\n","285 a nice personalized person on a bmx bike . a biker rides on a dirt road to get his .\n","286 a mother dog was giving her puppy some milk . a dog breastfeeding with .\n","287 a small and small brown puddle running through a grassy yard .\n","288 a skateboarder is on a city street rollerblader in red shirt is skating on a handrail and his .\n","289 a dog with a floppy ears runs in a dry field . a long eared dog is considered to .\n","290 a black and white lab with tags in the water . a black dog running in the surf trying to the water .\n","291 a girl was wearing a white coat standing in a fountain . a girl was singing in the middle of the mall when the sprinklers .\n","292 boy in green tunic on green tunic on rock wall . a little boy with a green shirt is running on a rock ledge his .\n","293 snowboarders do tricks on mountain .\n","294 two people can walk around outside while holding hands . two people walk out while holding hands .\n","295 smiling little girl swimming in outdoor pool . a little girl swimming in a pool and enjoying the magic .\n","296 the boy is in the red jersey runs with the football . 1 . 1 . 1 . 1 .\n","297 the boy who is in diving into the pool as the watch .\n","298 a dog is on an obstacle platform and a man . a man .\n","299 a girl came with a red helmet carting a lot of rope .\n"]}],"source":["with open(os.path.join(output_dir,\"reference_0_predictions_with_beam_search.txt\") ,\"w\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open(os.path.join(output_dir, \"./reference_0.txt\")) as fp:\n","        for line in fp:\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            print(c, get_best_sentence(out_sen, sentiment = 1))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VKTOYTjuPDJs"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-20-0105b5900bda\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reference_1_predictions_with_beam_search.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/transformer-drg-style-transfer-master/data/imagecaption/processed_files_with_bert_with_best_head/reference_1.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mout_sen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreditction_with_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/transformer-drg-style-transfer-master/data/imagecaption/processed_files_with_bert_with_best_head/reference_1.txt'"]}],"source":["with open(os.path.join(output_dir,\"reference_1_predictions_with_beam_search.txt\") ,\"w\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open(\"/content/drive/MyDrive/transformer-drg-style-transfer-master/data/imagecaption/processed_files_with_bert_with_best_head/reference_1.txt\") as fp:\n","        for line in fp:\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            print(c,get_best_sentence(out_sen, sentiment = 0))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"gxWgytu3PDJt"},"source":["## Delete, Retrieve and Generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0AKJa61PDJt"},"outputs":[],"source":["special_tokens = ['\u003cATTR_WORDS\u003e','\u003cCON_START\u003e','\u003cSTART\u003e','\u003cEND\u003e'] # Set the special tokens\n","tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt', special_tokens=special_tokens)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', num_special_tokens=len(special_tokens))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQAnDoZHPDJt"},"outputs":[],"source":["path = os.path.join(os.getcwd(), \"./log_yelp_retireve_edit_bert_best_head_preprocessing/pytorch_model_zero_grad_1.bin\")\n","model_state_dict = torch.load(path, map_location=device)\n","model.load_state_dict(model_state_dict)\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EF7yT7u8PDJt"},"outputs":[],"source":["# Output dir have reference files generated using TFIDF for retrieve attributes from opposite corpus\n","output_dir = \"/home/ubuntu/bhargav/data/yelp/processed_files_with_bert_with_best_head/delete_retrieve_edit_model/tfidf/\"\n","os.listdir(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5i0WeeHPDJt"},"outputs":[],"source":["with open(os.path.join(output_dir,\"reference_0_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open(os.path.join(output_dir, \"./reference_0_content.txt\")) as fp:\n","        for line in fp:\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            print(c,get_best_sentence(out_sen, sentiment = 1))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 1) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"executionInfo":{"elapsed":304,"status":"error","timestamp":1681881430089,"user":{"displayName":"Riya Tasgaonkar","userId":"15945572355651300701"},"user_tz":420},"id":"6HUw2k6gPDJt","outputId":"05d3c214-1e07-4cdf-a713-11f008aebefe"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-71-8facc0d68110\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reference_1_content.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mout_sen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreditction_with_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/transformer-drg-style-transfer-master/data/imagecaption/processed_files_with_bert_with_best_head/reference_1_content.txt'"]}],"source":["with open(os.path.join(output_dir,\"reference_1_predictions_with_full_sentence_match_beam_search_bm5.txt\") ,\"w\", encoding='utf-8') as out_fp:\n","    c = 0\n","    with open(os.path.join(output_dir, \"reference_1_content.txt\")) as fp:\n","        for line in fp:\n","            out_sen = preditction_with_beam_search(line.strip(), beam_width=5, vocab_length=max(tokenizer.special_tokens.values()) + 1)\n","            print(c,get_best_sentence(out_sen, sentiment = 0))\n","            c += 1\n","            out_fp.write(get_best_sentence(out_sen, sentiment = 0) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkX8lNlh7_bp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04a624acc17547118bdcc370eada2ffa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06ea2e0a15704143b4d4cba5bce4bf08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a50d26970e142f1a6bf3b7f8ed4434b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a73896714d241a3b07efc0de57910c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1900c2c62fc3473396da5a48f8033b9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c5e2beeea454c7a9eea633324eb3fda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_501c833c4a004b399da8debc5a86143a","placeholder":"​","style":"IPY_MODEL_0a73896714d241a3b07efc0de57910c3","value":" 481/481 [00:00\u0026lt;00:00, 22.4kB/s]"}},"20c0fa83f5f24857ad58dbaa55a5f305":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25a85c244ef247a4ac02c40cdf85c3b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28fa6e43850641a8a30f720f4855a20f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4593499c81d34f6ebdedec39c260e278","placeholder":"​","style":"IPY_MODEL_57cc482495d54fa39762f48c919cb70b","value":" 456k/456k [00:00\u0026lt;00:00, 716kB/s]"}},"2977503e80e74f7eb76228db79d79341":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"308573a128d74fcd96c03d47dea864a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31d15244ef294a579134e2d5387bd8eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a01481906cec4c288ba398830cc29c6b","placeholder":"​","style":"IPY_MODEL_997fbd2b8b5c4b0f9d77f809d93c9e56","value":"Downloading (…)olve/main/vocab.json: 100%"}},"370bf13637a348d1b649a6e167b81802":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a9298a6cdb141db922da3d8dfc27384":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ddc9efea6f84808bce5717910c63d2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"406c6bc2ba624879a4849d03158fd678":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4593499c81d34f6ebdedec39c260e278":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46414b09ff854083aa9be7995b4d8d8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a9298a6cdb141db922da3d8dfc27384","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_770b4fe72cff45c59ae8adf722b231a5","value":501200538}},"467ee0f1aca84e3d9fad47d410418c05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46e46b6013224543bceaeca6db830840":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_406c6bc2ba624879a4849d03158fd678","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04a624acc17547118bdcc370eada2ffa","value":481}},"501c833c4a004b399da8debc5a86143a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57cc482495d54fa39762f48c919cb70b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a0040cc92ad403380dae15d7aacfe22":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5efc7c41d29a4485be5e484373f2edb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7d7c09ad33443680cf9cc5f62ce539":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31d15244ef294a579134e2d5387bd8eb","IPY_MODEL_faf21745b8c64cd98d2943ddba007c0c","IPY_MODEL_b85fbfe17d334c1f862efb76c86668fb"],"layout":"IPY_MODEL_25a85c244ef247a4ac02c40cdf85c3b4"}},"62c64814e8ff4dc0bdf013708c61bf28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6edd9e3b79c34d00903f26e0047ae259":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"770b4fe72cff45c59ae8adf722b231a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82b88c8cd8dd4428942c6dcdaf618e98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93827b106e904c5aacd4fd88bbb02ad6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a50d26970e142f1a6bf3b7f8ed4434b","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62c64814e8ff4dc0bdf013708c61bf28","value":456318}},"98e075bd4ff34606823046d1864ea8ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6744c8628114ffe8e59f82f411d4351","placeholder":"​","style":"IPY_MODEL_20c0fa83f5f24857ad58dbaa55a5f305","value":"Downloading pytorch_model.bin: 100%"}},"997fbd2b8b5c4b0f9d77f809d93c9e56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a01481906cec4c288ba398830cc29c6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6974777bcfc402698bb0eaea9fa5415":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98e075bd4ff34606823046d1864ea8ca","IPY_MODEL_46414b09ff854083aa9be7995b4d8d8e","IPY_MODEL_d49eafd6269c45d38b2c0a3d537f9f99"],"layout":"IPY_MODEL_5efc7c41d29a4485be5e484373f2edb6"}},"b6744c8628114ffe8e59f82f411d4351":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85fbfe17d334c1f862efb76c86668fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a0040cc92ad403380dae15d7aacfe22","placeholder":"​","style":"IPY_MODEL_06ea2e0a15704143b4d4cba5bce4bf08","value":" 899k/899k [00:00\u0026lt;00:00, 1.40MB/s]"}},"b9689bb3ef7b46b1aa6b73d1b7aa7ebd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c451a9504564460faaf3776f570e375b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7c43f97410f4f86a0e75169bf2e91b4","IPY_MODEL_93827b106e904c5aacd4fd88bbb02ad6","IPY_MODEL_28fa6e43850641a8a30f720f4855a20f"],"layout":"IPY_MODEL_b9689bb3ef7b46b1aa6b73d1b7aa7ebd"}},"c7c43f97410f4f86a0e75169bf2e91b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2977503e80e74f7eb76228db79d79341","placeholder":"​","style":"IPY_MODEL_82b88c8cd8dd4428942c6dcdaf618e98","value":"Downloading (…)olve/main/merges.txt: 100%"}},"d49eafd6269c45d38b2c0a3d537f9f99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc8fb69dc3274f87a96e0db96d91021a","placeholder":"​","style":"IPY_MODEL_6edd9e3b79c34d00903f26e0047ae259","value":" 501M/501M [00:02\u0026lt;00:00, 75.5MB/s]"}},"d8646cf0e644481b95476ddbd37b45fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa2d5dcb7c0442ba8c5fc334539e5670","IPY_MODEL_46e46b6013224543bceaeca6db830840","IPY_MODEL_1c5e2beeea454c7a9eea633324eb3fda"],"layout":"IPY_MODEL_370bf13637a348d1b649a6e167b81802"}},"fa2d5dcb7c0442ba8c5fc334539e5670":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1900c2c62fc3473396da5a48f8033b9a","placeholder":"​","style":"IPY_MODEL_308573a128d74fcd96c03d47dea864a2","value":"Downloading (…)lve/main/config.json: 100%"}},"faf21745b8c64cd98d2943ddba007c0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ddc9efea6f84808bce5717910c63d2c","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_467ee0f1aca84e3d9fad47d410418c05","value":898823}},"fc8fb69dc3274f87a96e0db96d91021a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}